# é›¶åŸºç¡€å…¥é—¨å¤§æ¨¡å‹



## å‰è¨€



æœ¬ä¹¦çš„è®¾è®¡æœ‰ä»¥ä¸‹**äº”å¤§ç‰¹è‰²**ï¼š

- ä»æ„Ÿæ€§è®¤è¯†å‡ºå‘ â€” æ— éœ€ä»»ä½•å¤§æ¨¡å‹çŸ¥è¯†ï¼Œæ‰‹æŠŠæ‰‹å¸¦ä½ æ‰‹æ“ä¸€ä¸ªè‡ªå·±çš„æ¨¡å‹

- ä»æœ€ç»å…¸çš„GPTæ¨¡å‹å‡ºå‘ â€” å¸¦ä½ æ·±å…¥GPT2çš„å„ç§ç»†èŠ‚ï¼ŒæŒæ¡å¤§æ¨¡å‹çš„æ ¸å¿ƒåŸç†
- ä»æœ€å…ˆè¿›çš„æ¨¡å‹å‡ºå‘ â€”ã€€æ·±å…¥qwen2.5çš„å‚æ•°ï¼Œè®©ä½ å¯ä»¥åŸºäºå„ç±»åœºæ™¯ï¼Œè¿›è¡Œå¾®è°ƒ
- ä»æœ€å®ç”¨çš„å·¥å…·å‡ºå‘ã€€â€” ã€€ä½¿ç”¨ç›®å‰ä¸šå†…å¹¿æ³›ä½¿ç”¨çš„å¾®è°ƒå·¥å…· Llama-factory , è®©ä½ å¿«é€Ÿå¾®è°ƒå„ç±»æ¨¡å‹
- ä»é«˜é˜¶å‡ºå‘ã€€â€”ã€€åˆ©ç”¨å…ƒåç©ºå‡é˜¶ã€‚æ¯”å¦‚ï¼ŒGPT2ä½¿ç”¨çš„æ˜¯ä¼ ç»Ÿçš„MHAï¼Œåœ¨è¿›é˜¶éƒ¨åˆ†æˆ‘ä¼šæ€»ç»“MHAçš„ä¸è¶³ï¼Œè¿˜æœ‰å“ªäº›æ¨¡å‹ï¼Œè®©ä½ æ— ç¼è¡”æ¥åˆ°Qwen2.5ä½¿ç”¨çš„GQAã€‚





æœ¬ä¹¦æœ‰ä¸¤ä¸ªçº¿ç´¢ï¼Œä¸€æ¡æ˜¯ä¸»çº¿ï¼Œæ˜¯å¸¦é¢†æ·±å…¥æœ€ç»å…¸çš„GPT2æ¨¡å‹ï¼Œå¸¦ä½ æ‰‹æŠŠæ‰‹è®­ç»ƒä¸€ä¸ªGPTï¼’ã€‚ã€€ä¸€æ¡æ˜¯æš—çº¿ï¼Œé€šè¿‡åŸºç¡€åŸç†çš„è®²è§£ï¼Œä»¥åŠé«˜é˜¶æ€è€ƒä¸­çš„å…ƒåç©ºå‡é˜¶ï¼Œæ”¯æ’‘ä½ å»æ¢ç´¢qwen2.5ä»¥åŠåˆ«çš„æ¨¡å‹ï¼Œç†è§£å®ƒä»¬çš„åˆ›æ–°ä¹‹å¤„ã€‚ åœ¨è¿™ä¸ªæ—…é€”ä¹‹ä¸­ï¼Œä½ ä¼šæŒæ¡å¤§æ¨¡å‹æœ€æ ¸å¿ƒçš„åŸç†ï¼Œä¸€å¥—å¾®è°ƒçš„å·¥å…·å’Œæ–¹æ³•è®ºã€‚





### å¦‚ä½•ä½¿ç”¨æœ¬ä¹¦

### å¦‚ä½•è®­ç»ƒä¸€ä¸ªå¤§æ¨¡å‹ï¼Ÿ 

#### pretrain

#### rlhf

#### sft

### qwen2.5





#### æ¨¡å‹æ¶æ„

#### åŸºæœ¬å‚æ•°

#### ä½¿ç”¨vllméƒ¨ç½²qwen2.5



### pytorchå¸¸ç”¨è¯­æ³•

 

#### ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ“ä½œ





#### å¼ é‡çš„åŸºæœ¬æ“ä½œ

æ·±åº¦å­¦ä¹ ä¸­çš„è¾“å‡ºå‚æ•°å’Œè¾“å‡ºå‚æ•°æœ¬è´¨ä¸Šéƒ½æ˜¯å¼ é‡ã€‚é€šè¿‡äº†è§£å¼ é‡çš„å˜åŒ–ï¼Œäº†è§£æ¨¡å‹è¿›è¡Œä½•ç§è½¬åŒ–ã€‚

- åŸºæœ¬å±æ€§

```
x = torch.randn(3, 4, 5)

# å½¢çŠ¶
print(x.shape)  # torch.Size([3, 4, 5])
print(x.size())  # åŒä¸Š

# ç»´åº¦æ•°é‡
print(x.dim())  # 3

# æ•°æ®ç±»å‹
print(x.dtype)  # torch.float32

# è®¾å¤‡ä½ç½®
print(x.device)  # cpu æˆ– cuda:0

# æ€»å…ƒç´ æ•°é‡
print(x.numel())  # 3 * 4 * 5 = 60
```

- å†…å­˜ç›¸å…³

```
# æ˜¯å¦è¿ç»­å­˜å‚¨
print(x.is_contiguous())  

# æ˜¯å¦éœ€è¦æ¢¯åº¦
print(x.requires_grad)  

# è·å–æ¢¯åº¦
print(x.grad)  

# æŸ¥çœ‹å­˜å‚¨ä¿¡æ¯
print(x.storage())

```

- ç»´åº¦å˜æ¢

  ```
  # ç»´åº¦å˜æ¢
  x = x.view(12, 5)      # æ”¹å˜å½¢çŠ¶ï¼Œè¦æ±‚è¿ç»­
  x = x.reshape(12, 5)   # æ”¹å˜å½¢çŠ¶ï¼Œæ›´çµæ´»
  
  # ç»´åº¦è½¬ç½®
  x = x.transpose(0, 1)  # äº¤æ¢æŒ‡å®šç»´åº¦
  x = x.permute(2,0,1)   # ä»»æ„é¡ºåºé‡æ’ç»´åº¦
  
  # å¢å‡ç»´åº¦
  x = x.unsqueeze(0)     # å¢åŠ ç»´åº¦
  x = x.squeeze()        # ç§»é™¤å¤§å°ä¸º1çš„ç»´åº¦
  ```



- æ•°æ®è½¬åŒ–

```
# è®¾å¤‡è½¬æ¢
x = x.to('cuda')       # è½¬åˆ°GPU
x = x.cpu()           # è½¬åˆ°CPU

# ç±»å‹è½¬æ¢
x = x.float()         # è½¬ä¸ºfloat
x = x.long()          # è½¬ä¸ºlong
x = x.bool()          # è½¬ä¸ºboolean

# è½¬numpy
numpy_array = x.numpy()
# numpyè½¬tensor
tensor = torch.from_numpy(numpy_array)
```



- å¸¸ç”¨ä¿¡æ¯è·å–

```
# æœ€å¤§æœ€å°å€¼
print(x.max())
print(x.min())

# å‡å€¼æ ‡å‡†å·®
print(x.mean())
print(x.std())

# ç´¢å¼•ç›¸å…³
print(x.argmax())     # æœ€å¤§å€¼ç´¢å¼•
print(x.argmin())     # æœ€å°å€¼ç´¢å¼•
```



## ä¸€ã€å¿«é€Ÿå¼€å§‹ â€”  è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªæ¨¡å‹

é‡‡ç”¨é¢„è®­ç»ƒ(pretrain)å’Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼Œsftï¼‰çš„æ–¹æ³•ï¼Œä»¥æœ€å°çš„æˆæœ¬å¤ç° `minimind-zero`, åŠ é€Ÿé€šå…³ä»æ¨¡å‹é¢„è®­ç»ƒï¼Œå¾®è°ƒï¼Œåˆ°éƒ¨ç½²ã€‚è·å–æ„Ÿæ€§
<!--more-->

---

### ğŸ’» ç¯å¢ƒå‡†å¤‡


- ä»¥ä¸‹æˆ‘çš„è½¯ç¡¬ä»¶é…ç½®ï¼š
  - `windows10` 
  - `anaconda`
  - `python10.6`
  - `GPU`äº‘æœåŠ¡å¹³å°ï¼š[modal](https://modal.com/)

1. å…‹éš†é¡¹ç›®

```
git clone https://github.com/jingyaogong/minimind.git
```

  å®‰è£…ä¾èµ–, è¿›å…¥`minimind`é¡¹ç›®ã€‚ ä½¿ç”¨ `anaconda`æˆ–è€… `uv`çš„åŒå­¦ï¼Œå¯ä»¥å…ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå†å®‰è£…ã€‚

```
(minimind) D:\minimind>pip install -r requirements.txt
```

2. é…ç½® `modal` 

 `modal`  æ˜¯ä¸€ä¸ªGPUäº‘æœåŠ¡å¹³å°ã€‚å¦‚æœä½ æœ¬åœ°æ²¡æœ‰ `GPU`, å¯ä»¥ä½¿ç”¨äº‘æœåŠ¡å¹³å°æ¥è®­ç»ƒæ¨¡å‹ã€‚ 

æ–°æ‰‹å…¥é—¨ï¼Œå››æ­¥èµ°å¦‚ä¸‹ï¼š

- é¦–å…ˆï¼Œåœ¨å®˜ç½‘æ³¨å†Œã€‚å®˜ç½‘åœ°å€ï¼š https://modal.com/

- æœ¬åœ°å®‰è£… `modal` çš„å®¢æˆ·ç«¯ï¼Œé…ç½® `key` ã€‚æœ‰`python`ç¯å¢ƒå’Œ`pip`åŒ…çš„è¯ï¼Œä»¥ä¸‹ä¸¤ä¸ªå‘½ä»¤æå®š

```
pip install moda
python -m modal setup
```

æ³¨æ„ï¼šéœ€è¦æå‰å®‰è£… `pip`  å’Œ `python` ã€‚ 

- ç¼–å†™è„šæœ¬æ–‡ä»¶ ã€‚åœ¨è„šæœ¬æ–‡ä»¶ä¸­ç¼–å†™æ‰§è¡Œè®­ç»ƒåŠŸèƒ½çš„å‡½æ•°ï¼Œé…ç½®è®­ç»ƒæ—¶çš„ç›¸å…³å‚æ•°ï¼Œæ¯”å¦‚ï¼š`GPU`è°ƒç”¨ï¼Œæ–‡ä»¶å­˜å‚¨ç­‰ã€‚
- ä½¿ç”¨ `modal` å‘½ä»¤ï¼Œæ‰§è¡Œè„šæœ¬ä¸­ç›¸å…³çš„å‡½æ•°ã€‚

![img](https://d41chssnpqdne.cloudfront.net/user_upload_by_module/chat_bot/files/59476626/kEWGm3ZmXKHKnvcY.png?Expires=1743685818&Signature=OMHlacDQErvSa7wnB4ifIorqCDpmt4DtA34Qce0hcM111ugBJ~dwSFdurk61SQpC7cwEQ~uQUyMOScEkivoz1Cvz6VynJxUu~hbBATDeOpdfKQSWg4gPbBLSORmT3I2qk5n8hMxEEGpGqRm5ttYvIeKGj2cH5o6zPH0-R2PeZs9~KlfwiuKhBE7rfRLCAPfXTD6mxpsMyz2BagA34G1Bp~3TAqp0M8fV0ZJGLo5BM98hak7t215-wjCP22Rb9kqeJ8P780b9Zk8kcnZ7OK367Vv46DO14N5SYug1biXeGxLPw3p76Sd0NoBAZ~kvn~lcnMyKndu-l1pQ2dGcK9ctWw__&Key-Pair-Id=K3USGZIKWMDCSX)

**tips**:  æ³¨å†Œ `modal` èµ é€5$ï¼Œä¸å¤ªå¤Ÿç”¨ã€‚å¹¸è¿çš„æ˜¯ç»‘å®šä¿¡ç”¨å¡å¯ä»¥æ¯æœˆèµ é€30$. æˆ‘ç”¨çš„æ˜¯è™šæ‹Ÿä¿¡ç”¨å¡ï¼Œå‚è€ƒ:[nobepay](https://www.nobepay.com/)  æˆ–è€… `visa`å¡ã€‚



4. `modal` è„šæœ¬

 `modal`è„šæœ¬å‘½åä¸ºï¼š `modal.train.py` , æ”¾åœ¨ `minimind` çš„æ ¹ç›®å½•ä¸‹é¢ã€‚ä¸‹é¢ä¸ºè®­ç»ƒ `minidmind-zero` ç”¨åˆ°çš„

è„šæœ¬æ–‡ä»¶ã€‚ 

ä¸»è¦è¿›è¡Œçš„æ“ä½œä¸ºï¼šå®šä¹‰é•œåƒï¼Œå®‰è£…ç›¸å…³çš„ä¾èµ–, å¯¼å…¥æ•°æ®é›†æ–‡ä»¶ï¼›åˆ›å»º `volume` , å­˜å‚¨è®­ç»ƒåçš„æ¨¡å‹æ–‡ä»¶;å®šä¹‰ `pretrain`  `sft` çš„è®­ç»ƒå‡½æ•°ï¼ŒåŒ…æ‹¬ï¼šæ‰§è¡Œ `minimind`ä¸­çš„è„šæœ¬ï¼Œå®šä¹‰æ‰§è¡Œç›¸å…³çš„å‚æ•°ã€‚

```
import modal
from datetime import datetime  
app = modal.App("minimind-training")

# å®šä¹‰é•œåƒ
training_image = (modal.Image.debian_slim()
    .pip_install([
     
    ])
    .apt_install(
    "git"  # æ·»åŠ  git å®‰è£…
       )
    .run_commands([
        "git clone https://github.com/jingyaogong/minimind /root/minimind",
        "cd /root/minimind && pip install -r requirements.txt",
         "mkdir -p /root/minimind/out"  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ])
    .add_local_dir("dataset", remote_path="/root/minimind/dataset")  # ä½¿ç”¨add_local_dir
)


# ä½¿ç”¨volumeç¡®ä¿æ•°æ®æŒä¹…åŒ–
volume  = modal.Volume.from_name("my-volume")  # my-volume æ›¿æ¢ä¸ºä½ åˆ›å»ºçš„volumeåç§°

@app.function(
    image=training_image,
    gpu="A100",  # è¯·æ±‚ä½¿ç”¨ GPU
    timeout=3600,  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º1å°æ—¶
    max_containers=1,  
    volumes={"/root/minimind/out": volume}
)
def train_pretrain():
    import os
    os.chdir("/root/minimind")
    os.system("python train_pretrain.py")


@app.function(
    image=training_image,
    gpu="A100",  # è¯·æ±‚ä½¿ç”¨ GPU
    timeout=3600,  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º1å°æ—¶
    max_containers=1,  
    volumes={"/root/minimind/out": volume}
)
def train_sft():

    import os    
    volume.reload()
    os.chdir("/root/minimind")
    os.system("python train_full_sft.py")

```

å¯¹äºè®­ç»ƒå‡½æ•°çš„è®¾ç½®ï¼Œå¯ä»¥æ ¹æ®è®­ç»ƒå‡½æ•°ï¼Œè°ƒæ•´ `gpu` ç±»å‹å’Œ `timeout`ã€‚æ³¨æ„ï¼Œå°½å¯èƒ½è®¾ç½® `timeout` é•¿ä¸€äº›ï¼Œå¦åˆ™ä¼šå®¹æ˜“è¶…æ—¶ã€‚

```
@app.function(
    image=training_image,
    gpu="A100",  # è¯·æ±‚ä½¿ç”¨ GPU
    timeout=3600,  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º1å°æ—¶
    max_containers=1,  
    volumes={"/root/minimind/out": volume}
)
```



`my-volume` æ˜¯ä½ åˆ›å»ºçš„ `volume`åç§°ï¼Œä»https://modal.com/storageä¸­æŸ¥çœ‹ï¼Œéœ€è¦æå‰åˆ›å»ºï¼Œå‘½ä»¤å¦‚ä¸‹ã€‚

```
% modal volume create my-volume
Created volume 'my-volume' in environment 'main'.
```



5. æ•°æ®é›†

éœ€è¦ç”¨åˆ° `pretrian` å’Œ `sft`  å„è‡ªçš„æ•°æ®é›†.  ä¸‹è½½åœ°å€: https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files 

ä»…éœ€è¦ä¸‹è½½ `pretrain_hq.jsonl` å’Œ `sft_mini_512.jsonl`ï¼Œæ”¾åˆ°é¡¹ç›® `miminid/dataset` ä¸‹ã€‚

åœ¨è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬å°† `dataset` æŒ‚è½½è¿› å®¹å™¨çš„ `/root/minimind/dataset` ç›®å½•ã€‚

```
.add_local_dir("dataset", remote_path="/root/minimind/dataset")  # ä½¿ç”¨add_local_dir
```



è‡³æ­¤ï¼Œç›¸å…³ç¯å¢ƒå·²ç»å‡†å¤‡å¥½äº†ã€‚ä¸‹é¢å³å°†è¿›å…¥æ¿€åŠ¨äººå¿ƒçš„è®­ç»ƒç¯èŠ‚ã€‚åœ¨æ­¤æ¬¡è¯·æ£€æŸ¥ï¼š

- æˆåŠŸå®‰è£… `modal` å®¢æˆ·ç«¯
- `modal` ä¸Šåˆ›å»ºäº† `volume`ï¼Œ`modal.train.py`ä¸­çš„`volume`åç§°è¦æ›¿æ¢æˆä½ æ‰€åˆ›å»ºçš„ `volume` åç§°
- `minimind`æ ¹ç›®å½•ä¸‹å­˜åœ¨ `modal.train.py` è„šæœ¬
- `minimind/dataset` å†…å­˜åœ¨`pretrain_hq.jsonl` å’Œ `sft_mini_512.jsonl`è¿™ä¸¤ä¸ªæ•°æ®é›†ã€‚

---


### ğŸš€ æ¨¡å‹è®­ç»ƒ


é€šè¿‡**ç»ˆç«¯**è¿›å…¥åˆ° `minimind`ç›®å½•

#### é¢„è®­ç»ƒ(pretrain) â€”â€” è®©æ¨¡å‹å­¦ä¹ æµ·é‡çŸ¥è¯†

æ‰§è¡Œé¢„è®­ç»ƒçš„è„šæœ¬.

```
 modal run modal_train.py::train_pretrain
```

![image-20250320232517933](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250320232517933.png)



ä»æ—¥å¿—è¾“å‡º,çœ‹å‡ºå¤šå°‘ä¸ªEpochï¼Œæ‰§è¡Œäº†å¤šå°‘æ­¥ï¼Œlosså€¼å’Œå­¦ä¹ ç‡ä¸ºå¤šå°‘ã€‚

![image-20250321000118852](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250321000118852.png)

é€šè¿‡ç”Ÿæˆçš„é“¾æ¥ï¼Œå¯ä»¥è¿œç¨‹è®¿é—®è®­ç»ƒæƒ…å†µ

![image-20250320232658106](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250320232658106.png)

ç›¸å…³çš„æŒ‡æ ‡,å¯ä»¥æŸ¥çœ‹è®­ç»ƒæ—¶çš„ç³»ç»Ÿçš„è¿è¡Œæƒ…å†µã€‚

![image-20250321001112385](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250321001112385.png)



è®­ç»ƒç»“æŸï¼Œå¯ä»¥ä»ä» `volume` æŸ¥çœ‹è®­ç»ƒç»“æœã€‚

![image-20250321165059127](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250321165059127.png)



#### ç›‘ç£å¾®è°ƒ(sft) â€” è®©æ¨¡å‹å­¦ä¼šè¯´è¯

æ‰§è¡Œå‘½ä»¤

```
 modal run modal_train.py::train_sft
```

![image-20250321171223179](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250321171223179.png)

ç»“æŸä¹‹åï¼Œå‡ºç°ç»“æœæ–‡ä»¶ã€‚

![image-20250322085053992](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322085053992.png)



ä¸‹è½½`volume`ä¸­çš„è®­ç»ƒç»“æœåˆ°æœ¬åœ° `\minimind\out`æ–‡ä»¶å¤¹ä¸­ã€‚

```
modal volume get my-volume full_sft_512.pth pretrain_512.pth
```

![image-20250322090720246](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322090720246.png)

è‡³æ­¤ï¼Œè®­ç»ƒç»“æŸï¼Œè®©æˆ‘ä»¬æ¥æµ‹è¯•ä¸€ä¸‹ã€‚

`minimind`é¡¹ç›®ä¸­æä¾›è¿è¡Œæ¨¡å‹çš„è„šæœ¬, ç›´æ¥æ‰§è¡Œå°±å¯ä»¥ã€‚æ‰§è¡Œä¹‹å‰å¯ä»¥çœ‹ä¸‹ç›¸å…³å‚æ•°ï¼Œæ¯”å¦‚ï¼š`model_mode` æŒ‡çš„æ˜¯æ‰§è¡Œä»€ä¹ˆæ¨¡å‹

```
 parser.add_argument('--model_mode', default=1, type=int,
                        help="0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹ï¼Œ2: RLHF-Chatæ¨¡å‹ï¼Œ3: Reasonæ¨¡å‹")
```


æµ‹è¯• `sft` æ¨¡å‹

```
python eval_model.py --model_mode 1
```



é€‰æ‹©æ‰‹åŠ¨æµ‹è¯•

![image-20250322094629054](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322094629054.png)

 é€‰æ‹©è‡ªåŠ¨æµ‹è¯• 

![image-20250322094857989](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322094857989.png)

###  ğŸŒ æ¨¡å‹éƒ¨ç½²

é€šè¿‡å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¸Šä¼  `huggingface`  ,å¹¶ä¸”é€šè¿‡`huggingface` çš„ `space` åˆ›å»ºè¿è¡Œæ¨¡å‹çš„`demo` 

https://huggingface.co/docs 

####  è½¬åŒ–ä¸ºHFæ ¼å¼

ç›®å‰æ˜¯çš„ `pytorch` çš„æ ¼å¼,  éœ€è¦å…ˆè½¬åŒ–æˆ `HF`æ ¼å¼ï¼Œæ‰èƒ½ä¸Šä¼ åˆ° `huggingface`ã€‚æ¨¡å‹æ–‡ä»¶åœ¨`minimind\out\full_sft_512.pth` ã€‚

æ ¼å¼é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢è¿™ç¯‡æ–‡ç« ã€‚https://mp.weixin.qq.com/s/HXMLPM2KNkO6Mah-4btaEQ

é¡¹ç›®ä¸­æä¾›äº†è½¬åŒ–çš„è„šæœ¬, åœ¨ `srcipts\convert_model.py` ã€‚

æºä»£ç 

```
if __name__ == '__main__':
    lm_config = LMConfig(dim=512, n_layers=8, max_seq_len=8192, use_moe=False)

    torch_path = f"../out/rlhf_{lm_config.dim}{'_moe' if lm_config.use_moe else ''}.pth"

    transformers_path = '../MiniMind2-Small'

    # convert torch to transformers model
    convert_torch2transformers(torch_path, transformers_path)
```

è¿™é‡Œä¿®æ”¹å°† `stf` ä¹‹åçš„æ¨¡å‹è¿›è¡Œè½¬åŒ–.**è¯·æ›¿æ¢æ¨¡å‹è·¯å¾„ï¼Œä¿®æ”¹ä½ çš„æ¨¡å‹åç§°**

```
if __name__ == '__main__':
    lm_config = LMConfig(dim=512, n_layers=8, max_seq_len=8192, use_moe=False)

   # torch_path = f"../out/rlhf_{lm_config.dim}{'_moe' if lm_config.use_moe else ''}.pth"

    transformers_path = '../MiniMind-zero'  # ä½ æƒ³è¦çš„åç§°
    torch_path = "../out/full_sft_512.pth"  # ä½ çš„.pthæ–‡ä»¶è·¯å¾„
    # convert torch to transformers model
    convert_torch2transformers(torch_path, transformers_path)

    # # convert transformers to torch model
    # convert_transformers2torch(transformers_path, torch_path)
```

æ‰§è¡Œè„šæœ¬ä¹‹å, åœ¨æ ¹ç›®å½•ä¸‹å‡ºç°`MimiMind-zero`æ–‡ä»¶å¤¹

![image-20250322201705561](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322201705561.png)

![image-20250327222336660](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327222336660.png)



####  ä¸Šä¼ åˆ° Huggingface 

æ–°å»ºæ¨¡å‹ï¼šhttps://huggingface.co/new-space

![image-20250322204200340](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250322204200340.png)

é€šè¿‡ç½‘é¡µç›´æ¥ä¸Šä¼ æ¨¡å‹ã€‚ï¼ˆä¸­é—´æŠ˜è…¾è¿‡git, huggingface-cliï¼Œå‘ç°ç›´æ¥åœ¨**ç½‘é¡µä¸Šä¼ å‘æ˜¯æœ€å°‘**çš„ï¼‰

![image-20250323074542851](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250323074542851.png)

ä¿®æ”¹ `readme` æ–‡ä»¶ï¼Œæ·»åŠ ç›¸åº”çš„æ ‡ç­¾

```
language: zh
tags:
  - pytorch
  - gpt
  - transformers
  - text-generation-inference
library_name: transformers
pipeline_tag: text-generation
inference: true
```

è‡³æ­¤ä½ çš„æ¨¡å‹å·²ç»ä¸Šä¼ å¥½äº†ã€‚

https://huggingface.co/cmz1024/minimind-zero/tree/main

![image-20250327222957713](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327222957713.png)

####   åˆ›å»ºSpaceï¼Œshowå‡ºä½ çš„æ¨¡å‹

åˆ›å»ºspace,https://huggingface.co/new-space.æˆ‘é€‰æ‹©çš„æ˜¯é€šè¿‡ `Gradio` åˆ›å»ºï¼Œå¹¶å‹¾é€‰`chatbot`ã€‚

![image-20250327224010360](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327224010360.png)



åˆ›å»ºæˆåŠŸä¼šè¿›æ¥è¿™é‡Œ

![image-20250327224103524](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327224103524.png)

![image-20250327224139074](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327224139074.png)

è¿›å…¥çš„ `app.py` ï¼Œå‘ç°æ˜¯é€šè¿‡ `InferenceClient` æ¥è°ƒç”¨æ¨¡å‹ã€‚è¿™æ˜¯ `HF` æä¾›çš„ `Inference API` ã€‚ä½†æ˜¯å’±ä»¬çš„æ¨¡å‹æ˜¯åŸºäº `pytorch`æ„å»ºï¼Œè¦ä½¿ç”¨`Inference API` å¿…é¡»è¦ç”¨ `transformers` æ¥æ„å»ºã€‚

```
import gradio as gr
from huggingface_hub import InferenceClient

"""
For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference
"""
client = InferenceClient("HuggingFaceH4/zephyr-7b-beta") # hugginface æä¾›çš„Inference API
```

å½“ç„¶ä¹Ÿå¯ä»¥è½¬åŒ–ï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘é‡‡ç”¨ç›´æ¥åœ¨ `Gradio` åŠ è½½ `pytorch` æ¨¡å‹ã€‚

å¤§å®¶å¯ä»¥åˆ°è¿™é‡Œ[ä¸‹è½½ä»£ç ](https://cnb.cool/huoshuiai/LLM101-chenmuzhi/-/blob/main/app.py)ï¼Œæ›¿æ¢`app.py`ï¼Œ å…¶ä¸­è¦ä¿®æ”¹çš„æ˜¯ä½ çš„æ¨¡å‹è·¯å¾„ã€‚

```
# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "cmz1024/minimind-zero"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
```



è‡³æ­¤ï¼Œä½ å°±åˆ›å»ºäº†ä¸€ä¸ªå±•ç¤ºdemo, æŠŠä½ çš„å–œæ‚¦å’Œè¿›æ­¥åˆ†äº«ç»™ä½ çš„æœ‹å‹å§ã€‚

![image-20250327201730558](https://for-note.oss-cn-shanghai.aliyuncs.com/img/image-20250327201730558.png)







### å¿«é€Ÿå¼€å§‹çš„è¿›é˜¶æ€è€ƒ 





### å°ç»“

## äºŒã€æ•°æ®é›†



### å¦‚ä½•å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼ŸTokenizer 



æ•°æ®å¤„ç†çš„ç¬¬ä¸€æ­¥ï¼Œå°±æ˜¯ä½¿ç”¨æ•°å­—å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ 

Tokenizerï¼Œåˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬åˆ’åˆ†æˆä¸€ä¸ªä¸ªæœ€å°å•å…ƒ â€”è¯å…ƒ(token),  ç”¨äºæ¨¡å‹è®­ç»ƒã€‚

ä»¥BERTã€GPTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸ºä¾‹ï¼Œtokenizerçš„ä½œç”¨æµç¨‹å¦‚ä¸‹ï¼š

1. è¾“å…¥æ–‡æœ¬ï¼š"I love AI."
2. åˆ†è¯å™¨å¤„ç†åï¼š["I", "love", "AI", "."]
3. å†è½¬æˆIDï¼š[101, 2001, 727, 102]ï¼ˆå‡è®¾çš„IDï¼‰
4. è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚

è¯è¡¨(Vocabulary)æ˜¯åˆ†è¯å™¨çš„é‡è¦ç»„ä»¶ï¼Œå°†tokenè½¬åŒ–æˆtoken_idã€‚åœ¨è®­ç»ƒå‰çš„è¯­æ–™å¤„ç†ï¼Œæ¨¡å‹è¾“å‡ºçš„è§£ç é˜¶æ®µï¼Œéƒ½ä¼šç”¨åˆ°è¯è¡¨ã€‚ è¯è¡¨é‡è¦å‚æ•°å°±æ˜¯è¯è¡¨å¤§å°ï¼Œè¯è¡¨è¶Šå¤§ï¼Œèƒ½ç”Ÿæˆçš„è¯­è¨€è¶Šå¤šã€‚

å¸¸è§çš„åˆ†è¯å™¨æœ‰BPEã€wordPieceç­‰ã€‚ BPEåœ¨å¤§æ¨¡å‹å¹¿æ³›ä½¿ç”¨ï¼Œè€ŒwordPieceåˆ™æ˜¯åœ¨Bertä¸­ä½¿ç”¨ã€‚ä¸¤è€…éƒ½å±äº

å­è¯åˆ†è¯å™¨ã€‚



**ä»€ä¹ˆæ˜¯å­è¯ï¼Ÿ**

å­è¯æ˜¯ä»‹äºè¯å’Œå­—ç¬¦ä¹‹é—´å•å…ƒã€‚ å‡å¦‚ä½¿ç”¨è¯ä½œä¸ºå•ä½æ¥åˆ’åˆ†æ–‡æœ¬çš„è¯ï¼Œé¢—ç²’åº¦å¤ªå¤§ï¼Œè¯è¡¨æ— æ³•å…¼å®¹æ‰€æœ‰çš„è¯æ±‡ã€‚ å®¹æ˜“å‡ºç°é™Œç”Ÿè¯ï¼Œä¹Ÿå°±æ˜¯OOVé—®é¢˜(out-of-vocabulary) ã€‚ å‡å¦‚ä½¿ç”¨å­—ç¬¦æ¥è¿›è¡Œåˆ†è¯ï¼Œé¢—ç²’åº¦å¤ªå°ï¼Œtokenæ²¡æœ‰å®é™…çš„å«ä¹‰ã€‚

å­è¯å…¼æœ‰ä¸¤è€…çš„ä¼˜ç‚¹ã€‚

æ¯”å¦‚ï¼š å¸¸è§è‹±æ–‡å•è¯çš„å‰ç¼€å’Œåç¼€ï¼Œreï¼Œfulï¼Œlyã€‚



**BPE(byte pair encoding)ç®—æ³•çš„åŸç† .**

å®ƒæ˜¯å¦‚ä½•è®­ç»ƒï¼Œæ„å»ºè¯è¡¨çš„å‘¢ï¼Ÿ

å…ˆå°†è¦è®­ç»ƒçš„æ–‡æœ¬é›†åˆï¼ŒæŒ‰ç…§å­—ç¬¦æ¥è¿›è¡Œæ‹†åˆ†ã€‚æ‹†åˆ†ä¹‹åï¼Œå°†ç›¸é‚»å­—ç¬¦ç»„åˆï¼Œæ„æˆå­è¯ï¼Œç»Ÿè®¡å­è¯çš„é¢‘ç‡ã€‚

å‡è®¾æ–‡æœ¬ä¸ºï¼Œâ€œï¼©ã€€love youâ€, é‚£ä¹ˆç»Ÿè®¡ I , Io,ov, yo, ou çš„é¢‘ç‡ã€‚

å°†é¢‘ç‡é«˜çš„å­è¯ï¼Œç¡®å®šè¿›è¡Œåˆå¹¶ã€‚ 

å†è¿›è¡Œä¸‹ä¸€è½®ç»Ÿè®¡ã€‚å°†å­è¯å’Œç›¸é‚»çš„å­—ç¬¦æˆ–è€…å­è¯åˆå¹¶ï¼Œåˆå¹¶é¢‘ç‡æœ€é«˜çš„ã€‚

ç›´åˆ°è¾¾åˆ°è¯è¡¨çš„å¤§å°ã€‚



**wordPieceåˆ†è¯çš„åŸç†ã€‚**

wordPieceæ˜¯å­è¯åˆ†è¯ç®—æ³•ï¼Œåœ¨Bertç­‰è¯­è¨€æ¨¡å‹ä¹‹ä¸­å¹¿æ³›ä½¿ç”¨ã€‚wordPieceå’ŒBPEçš„åŒºåˆ«åœ¨ä¸åœ¨è¿›è¡Œå­è¯åˆå¹¶çš„æ—¶å€™ï¼Œè€ƒè™‘äº†è¯­æ–™çš„æ¦‚ç‡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨åˆ†è¯ä¹‹åï¼Œè¦ç¡®å¸¸è§å­è¯çš„æ¦‚ç‡ã€‚è€ŒBPEä¸­ä»…æ ¹æ®å‡ºç°é¢‘ç‡æ¥è¿›è¡Œè€ƒè™‘ã€‚å‡ºç°é¢‘ç‡é«˜çš„ï¼Œä¸ä¸€å®šæ˜¯å¸¸è§å­è¯ã€‚

å‡è®¾æˆ‘ä»¬æœ‰å¦‚ä¸‹ç®€å•è¯­æ–™ï¼š

```
å¤åˆ¶unhappiness
unhappy
unhappily
unhappiest
```

**1. BPE çš„å¤„ç†æ–¹å¼**

BPE ä¼šç»Ÿè®¡æ‰€æœ‰è¿ç»­å­—ç¬¦å¯¹çš„å‡ºç°é¢‘ç‡ï¼Œæ¯”å¦‚ï¼š

- â€œunâ€ã€â€œhaâ€ã€â€œppâ€ã€â€œinâ€ã€â€œesâ€ã€â€œlyâ€ã€â€œestâ€ç­‰ç­‰

å‡è®¾â€œppâ€åœ¨è¯­æ–™ä¸­å‡ºç°é¢‘ç‡å¾ˆé«˜ï¼ˆæ¯”å¦‚åœ¨â€œhappinessâ€ã€â€œhappyâ€ã€â€œhappilyâ€ã€â€œhappiestâ€é‡Œéƒ½å‡ºç°äº†ï¼‰ï¼ŒBPE å¯èƒ½ä¼šä¼˜å…ˆæŠŠâ€œppâ€åˆå¹¶æˆä¸€ä¸ªå­è¯ã€‚

ä½†æœ‰æ—¶å€™ï¼ŒæŸäº›å­—ç¬¦å¯¹è™½ç„¶é¢‘ç‡é«˜ï¼Œå´å¹¶ä¸æ˜¯æœ‰å®é™…æ„ä¹‰çš„å­è¯ï¼ˆæ¯”å¦‚â€œppâ€æœ¬èº«åœ¨è‹±æ–‡ä¸­æ²¡æœ‰ç‹¬ç«‹æ„ä¹‰ï¼‰ã€‚

**2. WordPiece çš„å¤„ç†æ–¹å¼**

WordPiece ä¸ä»…è€ƒè™‘â€œppâ€çš„å‡ºç°é¢‘ç‡ï¼Œè¿˜ä¼šè®¡ç®—å¦‚æœæŠŠâ€œppâ€åˆå¹¶æˆä¸€ä¸ªå­è¯ï¼Œæ˜¯å¦èƒ½æ˜¾è‘—æå‡æ•´ä¸ªè¯­æ–™çš„æ¦‚ç‡ï¼ˆå³æ›´å¥½åœ°è¡¨ç¤ºåŸå§‹è¯­æ–™ä¸­çš„å•è¯ï¼‰ã€‚

å‡è®¾â€œunâ€ã€â€œhappyâ€ã€â€œnessâ€ã€â€œlyâ€ã€â€œestâ€è¿™äº›å­è¯åœ¨è‹±è¯­ä¸­å¾ˆå¸¸è§ï¼ŒWordPiece å¯èƒ½ä¼šæ›´å€¾å‘äºåˆå¹¶è¿™äº›æœ‰å®é™…æ„ä¹‰çš„å­è¯ï¼Œè€Œä¸æ˜¯ä»…ä»…é¢‘ç‡é«˜ä½†æ²¡æ„ä¹‰çš„â€œppâ€ã€‚

æ¯”å¦‚ï¼ŒWordPiece å¯èƒ½ä¼šä¼˜å…ˆå¾—åˆ°å¦‚ä¸‹å­è¯ï¼š

- â€œunâ€
- â€œhappyâ€
- â€œnessâ€
- â€œlyâ€
- â€œestâ€

è¿™æ ·ï¼Œâ€œunhappinessâ€ä¼šè¢«åˆ†æˆâ€œun + happy + nessâ€ï¼Œè€Œä¸æ˜¯â€œun + ha + pp + in + essâ€ã€‚

| å•è¯        | BPE åˆ†è¯ç»“æœ            | WordPiece åˆ†è¯ç»“æœ |
| ----------- | ----------------------- | ------------------ |
| unhappiness | un + ha + pp + in + ess | un + happy + ness  |
| unhappily   | un + ha + pp + ily      | un + happy + ly    |
| unhappy     | un + ha + pp + y        | un + happy         |



### embedding â€” æŠŠæ•°å­—å˜æˆå‘é‡

ç»è¿‡åˆ†è¯ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°æ–‡æœ¬å¯¹åº”çš„token_idï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸²æ•°å­—ã€‚



ä¸ºä»€ä¹ˆéœ€è¦æŠŠtoken_idè½¬åŒ–æˆå‘é‡ï¼Ÿ

token_id åªæ˜¯ä¸€ä¸ªæ•´æ•°ç¼–å·ï¼Œæœ¬èº«æ²¡æœ‰ä»»ä½•è¯­ä¹‰ä¿¡æ¯ã€‚ç¥ç»ç½‘ç»œæ“…é•¿å¤„ç†**å‘é‡**ï¼ˆä¸€ç»„æœ‰æ„ä¹‰çš„æ•°å­—ï¼‰ï¼Œè€Œä¸æ˜¯å•ä¸€çš„æ•´æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæœ‰è½¬åŒ–æˆå‘é‡ï¼Œæ‰èƒ½è¿›è¡Œè¯­ä¹‰è¡¨ç¤ºï¼Œæ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚



å¦‚ä½•è½¬åŒ–ï¼Ÿ

ä»£ç xx 



### å¦‚ä½•å¯¹ä½ç½®è¿›è¡Œç¼–ç ï¼Ÿ 

ä½ç½®ç¼–ç æ˜¯ä¸€ç§å°†



### æ„å»ºInput- targetè®­ç»ƒå¯¹



### å¸¸è§çš„æ•°æ®é›†æ ¼å¼

#### AIpaca



#### ShareGPT 





### æ•°æ®é›†çš„è¿›é˜¶æ€è€ƒ 

**å…ƒ**

**å**

**ç©º**

### å°ç»“

## ä¸‰ã€è‡ªæ³¨æ„åŠ›æœºåˆ¶ 



### ä¸ºä½•Transfomeræ¶æ„èƒ½å¤Ÿèƒœå‡ºï¼Ÿ



å…ˆçœ‹çœ‹transformerä¹‹å‰çš„RNNå’ŒCNN. 



RNNå’ŒLSTMæ˜¯ä»€ä¹ˆ ? 

1. RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼ŒRecurrent Neural Networkï¼‰

**åŸºæœ¬åŸç†**

- RNNæ˜¯ä¸€ç±»ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œã€‚
- ä¸åŒäºä¼ ç»Ÿçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼ŒRNNåœ¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šæ¥æ”¶å½“å‰è¾“å…¥å’Œä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„â€œéšè—çŠ¶æ€â€ä½œä¸ºè¾“å…¥ï¼Œå®ç°ä¿¡æ¯çš„â€œè®°å¿†â€ä¸ä¼ é€’ã€‚

**ä¼˜ç‚¹**

- èƒ½å¤„ç†å˜é•¿çš„åºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€æ—¶é—´åºåˆ—ç­‰ï¼‰ã€‚
- ç»“æ„ç®€å•ï¼Œå‚æ•°å…±äº«ï¼Œé€‚åˆåºåˆ—å»ºæ¨¡ã€‚

**ç¼ºç‚¹**

- **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼šé•¿åºåˆ—è®­ç»ƒæ—¶ï¼Œæ¢¯åº¦ä¼šè¿…é€Ÿå˜å°æˆ–å˜å¤§ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
- è®­ç»ƒæ•ˆç‡è¾ƒä½ï¼Œä¸èƒ½å¹¶è¡Œã€‚

LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ŒLong Short-Term Memoryï¼‰

**åŸºæœ¬åŸç†**

- LSTMæ˜¯RNNçš„æ”¹è¿›ç‰ˆï¼Œä¸“é—¨ä¸ºäº†è§£å†³RNNçš„â€œé•¿è·ç¦»ä¾èµ–â€é—®é¢˜ï¼ˆå³æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼‰ã€‚
- é€šè¿‡å¼•å…¥â€œé—¨æ§æœºåˆ¶â€ï¼Œæ§åˆ¶ä¿¡æ¯çš„â€œè®°å¿†â€ä¸â€œé—å¿˜â€ã€‚LSTMå•å…ƒåŒ…å«ä¸‰ä¸ªé—¨ï¼šè¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨ã€‚

| ç‰¹ç‚¹       | RNN               | LSTM                     |
| ---------- | ----------------- | ------------------------ |
| ç»“æ„       | ç®€å•              | å¤æ‚ï¼ˆæœ‰é—¨æ§ï¼‰           |
| é•¿è·ç¦»ä¾èµ– | å®¹æ˜“ä¸¢å¤±          | èƒ½è¾ƒå¥½æ•æ‰               |
| å‚æ•°é‡     | å°‘                | å¤š                       |
| è®­ç»ƒéš¾åº¦   | æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ä¸¥é‡ | ç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸        |
| è®¡ç®—é€Ÿåº¦   | æ…¢ï¼ˆä¸èƒ½å¹¶è¡Œï¼‰    | æ…¢ï¼ˆä¸èƒ½å¹¶è¡Œï¼‰           |
| åº”ç”¨       | ç®€å•åºåˆ—å»ºæ¨¡      | å¤æ‚åºåˆ—å»ºæ¨¡ï¼Œé•¿ä¾èµ–åœºæ™¯ |



transformerçš„ä¼˜åŠ¿åœ¨å“ªé‡Œï¼Ÿ  

1. å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼º
RNN/LSTMï¼šåºåˆ—æ•°æ®å¿…é¡»æŒ‰æ—¶é—´æ­¥ä¾æ¬¡å¤„ç†ï¼Œä¸èƒ½å¹¶è¡Œï¼ˆå³ç¬¬tæ­¥çš„è¾“å‡ºä¾èµ–äºç¬¬t-1æ­¥çš„è¾“å‡ºï¼‰ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ…¢ã€‚
Transformerï¼šåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ‰€æœ‰ä½ç½®çš„è¾“å…¥å¯ä»¥åŒæ—¶å¤„ç†ï¼Œå®ç°å®Œå…¨å¹¶è¡Œï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ã€‚
2. æ•æ‰é•¿è·ç¦»ä¾èµ–èƒ½åŠ›å¼º
RNN/LSTMï¼šè™½ç„¶LSTMé€šè¿‡é—¨æ§æœºåˆ¶ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½†é•¿è·ç¦»ä¾èµ–ä»ç„¶éš¾ä»¥æ•æ‰ï¼Œä¿¡æ¯ä¼ é€’è·¯å¾„é•¿ï¼Œå®¹æ˜“ä¸¢å¤±ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
Transformerï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç›´æ¥å»ºç«‹ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„è”ç³»ï¼Œæ— è®ºè·ç¦»å¤šè¿œï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–æ•ˆæœæ›´å¥½ã€‚
3. å»ºæ¨¡çµæ´»
RNN/LSTMï¼šåªèƒ½é¡ºåºå»ºæ¨¡ï¼Œéš¾ä»¥å¤„ç†éé¡ºåºç»“æ„çš„æ•°æ®ã€‚
Transformerï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥çµæ´»åœ°å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä½ç½®ä¹‹é—´çš„å…³ç³»ï¼Œæ›´é€‚åˆå¤æ‚ç»“æ„çš„æ•°æ®ã€‚
4. æ‰©å±•æ€§å¼º
RNN/LSTMï¼šå †å å±‚æ•°å—é™ï¼Œå±‚æ•°å¤šäº†æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚
Transformerï¼šç»“æ„ç®€å•ï¼Œæ˜“äºå †å æ›´æ·±çš„ç½‘ç»œå±‚ï¼Œæå‡æ¨¡å‹å®¹é‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚
5. æ›´é€‚åˆå¤§è§„æ¨¡æ•°æ®å’Œé¢„è®­ç»ƒ
Transformerç»“æ„éå¸¸é€‚åˆå¤§è§„æ¨¡æ•°æ®çš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œä¹Ÿæ˜¯BERTã€GPTç­‰é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ç»“æ„ã€‚
æ€»ç»“
RNN/LSTMï¼šé¡ºåºå¤„ç†ã€ä¾èµ–å‰åå…³ç³»ã€æ•æ‰é•¿è·ç¦»ä¾èµ–å¼±ã€ä¸æ˜“å¹¶è¡Œã€‚
Transformerï¼šå…¨å±€è‡ªæ³¨æ„åŠ›ã€å¹¶è¡Œå¤„ç†ã€æ•æ‰é•¿è·ç¦»ä¾èµ–å¼ºã€æ˜“æ‰©å±•å’Œé¢„è®­ç»ƒã€‚



### è‡ªæ³¨æ„åŠ›æœºåˆ¶

è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ  



è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§å¯¹**åŒä¸€åºåˆ—**å†…ä¸åŒtokenä¹‹é—´çš„å…³ç³»ç¼–ç çš„æŠ€æœ¯ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤Ÿå°†tokenå’Œtokenä¹‹é—´çš„å…³ç³»è¿›è¡Œç¼–ç ã€‚

é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåºåˆ—ä¸­çš„tokenéƒ½ä¼šå¾—åˆ°ä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext -vector), ç”¨äºè¡¨ç¤ºåºåˆ—ä¸­ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚



**ç®€å•çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶**ä¸åŒ…å«**æƒé‡çŸ©é˜µ**ï¼Œä»…ç”¨äºæ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•æµç¨‹ã€‚æƒé‡çŸ©é˜µæ˜¯æ¨¡å‹ä¸­å¯è®­ç»ƒçš„å‚æ•°ã€‚ 



ç®€å•çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„**å®ç°æµç¨‹**ï¼š

éå†åºåˆ—ä¸­çš„tokenæ‰€å¯¹åº”çš„å‘é‡å€¼ã€‚

è®¡ç®—å½“å‰å‘é‡å’Œå…¶å®ƒå‘é‡çš„ç‚¹ç§¯ï¼Œå¾—åˆ°æƒé‡åˆ†æ•°ï¼Œæƒé‡åˆ†æ•°ç»è¿‡ `softmax`å‡½æ•°å½’ä¸€åŒ–å¾—åˆ°æƒé‡å€¼ã€‚

åºåˆ—ä¸­çš„æ‰€æœ‰å‘é‡ * æƒé‡æ•°è¿›è¡Œç´¯åŠ ï¼Œå°±å¾—åˆ°å½“å‰tokenå¯¹åº”çš„ä¸Šä¸‹æ–‡å‘é‡ã€‚



ä¸¾ä¾‹ï¼š

åºåˆ—ä¸ºï¼šâ€æˆ‘çˆ±é˜…è¯»â€œ

å¯¹åº”çš„å‘é‡ï¼š(vector1, vetor2,vector3,vector4 )

å½“å‰çš„tokenä¸ºâ€æˆ‘â€œï¼Œé‚£ä¹ˆä¼šç”¨ vector1 *  vectori ï¼ˆi = 1,2,3,4ï¼‰å¾—åˆ°æƒé‡åˆ†æ•°(score1,score2,score3,score4)

æƒé‡åˆ†æ•°å½’ä¸€åŒ–ä¹‹åå¾—åˆ°(w1,w2,w3,w4)

æœ€ç»ˆå¾—åˆ°ï¼Œâ€æˆ‘â€œå¯¹åº”çš„ context_vector1 =   w1 *vecotr1 + w2 * vector2 + w3 * vector3 + w4* vector4ã€‚ 



åœ¨ç¬¬äºŒç« æ ¹æ®tokenåˆ›å»ºembedingçš„æ—¶å€™ï¼Œä»…è€ƒè™‘äº†å•ä¸ªtoeknä»¥åŠtokenåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œæ²¡æœ‰è€ƒè™‘tokenä¹‹é—´çš„å…³ç³»ã€‚

è‡ªæ³¨æ„åŠ›æœºåˆ¶å¼¥è¡¥äº†è¿™ä¸ªç¼ºç‚¹ï¼Œç»è¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¾—åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œèƒ½æ›´å¥½çš„è¡¨ç¤ºtokençš„ï¼Œè®©æ¨¡å‹è·å¾—æ›´å¥½åœ°è¾“å‡ºã€‚





#### æƒé‡åˆ†æ•°

```
import torch

inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)
```



```
# åˆ›å»ºä¸€ä¸ª 6x6 çš„é›¶å¼ é‡ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›åˆ†æ•°
attn_scores = torch.empty(6, 6)

# éå†è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ 
for i, x_i in enumerate(inputs):
    # å¯¹äºå½“å‰çš„è¾“å…¥å…ƒç´  x_iï¼Œå†æ¬¡éå†æ•´ä¸ªè¾“å…¥åºåˆ—
    for j, x_j in enumerate(inputs):
        # è®¡ç®— x_i å’Œ x_j çš„ç‚¹ç§¯ï¼Œä½œä¸ºæ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶å­˜å‚¨åœ¨ attn_scores çŸ©é˜µçš„å¯¹åº”ä½ç½®
        attn_scores[i, j] = torch.dot(x_i, x_j)

# æ‰“å°å®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
print(attn_scores)
```



æˆ–è€…å¯ä»¥é€šè¿‡çŸ©é˜µä¹˜æ³•è®¡ç®—

```
# ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—è¾“å…¥åºåˆ—çš„ç‚¹ç§¯çŸ©é˜µ
# inputs @ inputs.T ç›¸å½“äº inputs ä¸ inputs çš„è½¬ç½®ç›¸ä¹˜
attn_scores = inputs @ inputs.T

# æ‰“å°æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
print(attn_scores)
```

#### æƒé‡

softmaxvå½’ä¸€åŒ–ä¹‹åå¾—åˆ°æƒé‡ 

```
attn_weights = torch.softmax(attn_scores, dim=1)
print(attn_weights)
```



#### ä¸Šä¸‹æ–‡å‘é‡

```
all_context_vecs = attn_weights @ inputs
print(all_context_vecs)
```

#### QKVçŸ©é˜µ



```python
# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
torch.manual_seed(123)

# åˆ›å»ºæŸ¥è¯¢æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (d_in, d_out)ï¼Œå¹¶ä¸”è®¾ç½® requires_grad=False è¡¨ç¤ºè¿™äº›æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–°
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

# åˆ›å»ºé”®æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶å’Œ W_query ç›¸åŒï¼ŒåŒæ ·è®¾ç½® requires_grad=False
W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)

# åˆ›å»ºå€¼æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶å’Œ W_query ç›¸åŒï¼ŒåŒæ ·è®¾ç½® requires_grad=False
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
```



```
# ä½¿ç”¨é”®æƒé‡çŸ©é˜µ W_key å°†è¾“å…¥åºåˆ— inputs æŠ•å½±åˆ°é”®ç©ºé—´
keys = inputs @ W_key

# ä½¿ç”¨å€¼æƒé‡çŸ©é˜µ W_value å°†è¾“å…¥åºåˆ— inputs æŠ•å½±åˆ°å€¼ç©ºé—´
values = inputs @ W_value

# æ‰“å°é”®å‘é‡çš„å½¢çŠ¶
print("keys.shape:", keys.shape)

# æ‰“å°å€¼å‘é‡çš„å½¢çŠ¶
print("values.shape:", values.shape)
```



å®ç°ä¸€ä¸ªç´§å‡‘çš„selfAttentionç±» 

```python
import torch.nn as nn

# å®šä¹‰è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç¬¬äºŒä¸ªç‰ˆæœ¬
class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__()
        # è®¾ç½®è¾“å‡ºç»´åº¦
        self.d_out = d_out
        # åˆå§‹åŒ–æŸ¥è¯¢ã€é”®å’Œå€¼çš„çº¿æ€§å±‚ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦åŒ…å«åç½®é¡¹
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        # ä½¿ç”¨çº¿æ€§å±‚å°†è¾“å…¥ x æŠ•å½±åˆ°æŸ¥è¯¢ã€é”®å’Œå€¼ç©ºé—´
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
        attn_scores = queries @ keys.T
        
        # ä½¿ç”¨ softmax å‡½æ•°å’Œç¼©æ”¾å› å­å½’ä¸€åŒ–æ³¨æ„åŠ›åˆ†æ•°
        # æ³¨æ„è¿™é‡Œçš„ dim=1ï¼Œè¡¨ç¤ºæ²¿ç€é”®å‘é‡çš„ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)

        # ä½¿ç”¨å½’ä¸€åŒ–çš„æ³¨æ„åŠ›æƒé‡å’Œå€¼å‘é‡è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡
        context_vec = attn_weights @ values
        return context_vec

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
torch.manual_seed(789)
# åˆ›å»º SelfAttention_v2 å®ä¾‹
sa_v2 = SelfAttention_v2(d_in, d_out)
# ä½¿ç”¨è¾“å…¥æ•°æ® inputs è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶æ‰“å°ç»“æœ
print(sa_v2(inputs))
```



### å› æœæ³¨æ„åŠ›





#### å› æœæ³¨æ„åŠ›æ©ç 



å› æœæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åŸºç¡€ä¹‹ä¸Šï¼Œå¢åŠ å› æœæ³¨æ„åŠ›æ©ç å’Œ `dropout`æœºåˆ¶çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚



å› æœæ³¨æ„åŠ›æ©ç ä¸€ç§å‰”é™¤åºåˆ—ä¸­å¾€åçš„tokenå¯¹å½“å‰tokené¢„æµ‹å½±å“çš„æŠ€æœ¯, å®ƒèƒ½å¤Ÿè®©ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ä»…åŸºä¹‹å‰çš„tokenã€‚ 



å¦‚ä½•å®ç°å› æœæ³¨æ„åŠ›ï¼Ÿ 



åœ¨ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œ è®¡ç®—æƒé‡æ—¶ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªn *nçš„æƒé‡WçŸ©é˜µã€‚

ç¬¬iè¡Œè¡¨ç¤ºï¼Œç¬¬iä¸ªtokenå¯¹äºéœ€äº†ä¸­çš„å„ä¸ªtokençš„æƒé‡ã€‚ wï¼ˆi,jï¼‰è¡¨ç¤ºç¬¬iä¸ªtoken,å¯¹äºç¬¬jä¸ªtokençš„æƒé‡åˆ†æ•°ã€‚

å¯¹æƒé‡çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–å¤„ç†ï¼Œå°†å¯¹è§’çº¿ä¸Šæ–¹çš„ä½ç½®å¯¹ç½®ä¸º0ã€‚ å¦‚æ­¤ä¸€æ¥ï¼Œæ¯ä¸ªtokenåªèƒ½çœ‹åˆ°è‡ªå·±ä¹‹å‰çš„tokenã€‚

å¯¹è§’çº¿ä¸Šæ–¹ç½®ä¸º0ä¹‹åï¼Œæ¯ä¸€è¡Œçš„æ¦‚ç‡å’Œä¸å†ä¸º0ï¼Œéœ€é‡æ–°è¿›è¡Œ `softmax` å½’ä¸€åŒ–å¤„ç†ã€‚ 



ä»£ç å®ç°ï¼š

åˆ›å»ºä¸€ä¸ªæ©ç  ï¼š

```
# æˆ‘ä»¬åˆ›å»ºçš„æ©ç å½¢çŠ¶åº”è¯¥å’Œæ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„å½¢çŠ¶ä¸€è‡´ï¼Œä»¥ä¸€ä¸€å¯¹åº”
block_size = attn_scores.shape[0]
# tril æ–¹æ³•ä¼šåˆ›å»ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µ
mask_simple = torch.tril(torch.ones(block_size, block_size))
print(mask_simple)
```





```
masked_simple = attn_weights*mask_simple
print(masked_simple)
```



å†æ¬¡è¿›è¡Œå½’ä¸€åŒ– 

```
# dim = 1 è¡¨ç¤ºæŒ‰è¡Œæ±‚å’Œ
row_sums = masked_simple.sum(dim=1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)
```



å¦å¤–ä¸€ä¸ªæ€è·¯æ˜¯åœ¨softmaxä¹‹å‰ï¼Œç”¨è´Ÿæ— ç©·æ©ç›–å¯¹è§’çº¿ä»¥ä¸Šçš„éƒ¨åˆ†ã€‚

 

```
mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)
```



softmaxä¹‹åï¼Œè´Ÿæ— ç©·çš„éƒ¨åˆ†ï¼Œéƒ½æ˜¯0 

```
attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)
print(attn_weights)
```



#### dropoutæœºåˆ¶

 ä½¿ç”¨`dropout`  ï¼Œä¼šéšæœºæŒ‰ç…§ä¸€å®šçš„æ¯”ä¾‹ï¼Œå°†æƒé‡çŸ©é˜µç§çš„å…ƒç´ ç½®ä¸º0.



```
# éšä¾¿è®¾ç½®ä¸€ä¸ªéšæœºæ•°ç§å­
torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5) # è®¾ç½® 50% çš„ Dropout æ¯”ä¾‹
# å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œ dropout
print(dropout(attn_weights))
```



å®ç°ä¸€ä¸ªå› æœæ³¨æ„åŠ›ç±» 



```
# å®šä¹‰ä¸€ä¸ªå¸¦ dropout çš„å› æœè‡ªæ³¨æ„åŠ›å±‚
class CausalAttention(nn.Module):

    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):
        '''
        æ„é€ å‡½æ•°ï¼Œè¾“å…¥å‚æ•°å¦‚ä¸‹ï¼š
        d_in: è¾“å…¥çš„ç»´åº¦
        d_out: è¾“å‡ºçš„ç»´åº¦
        block_size: æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„å¤§å°
        dropout: dropout æ¯”ä¾‹
        qkv_bias: æ˜¯å¦å¯¹ queryã€key å’Œ value åŠ åç½®
        '''
        super().__init__()
        self.d_out = d_out
        # æ ¹æ®å‰æ–‡ï¼Œæ¯ä¸€ä¸ªæƒé‡çŸ©é˜µéƒ½æ˜¯ d_in x d_out çš„çº¿æ€§å±‚
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        # ä¸€ä¸ª dropout å±‚
        self.dropout = nn.Dropout(dropout) 
        # ä¸€ä¸ªæ©ç çŸ©é˜µï¼Œä¸‹ä¸‰è§’ä¸º 1ï¼Œå…¶ä½™ä¸º 0
        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1)) # New

    def forward(self, x):
        '''
        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œè¾“å…¥å‚æ•°ä¸º xï¼Œç»´åº¦ä¸º b x num_tokens x d_inï¼Œè¾“å‡ºç»´åº¦ä¸º b x num_tokens x d_out
        '''
        b, num_tokens, d_in = x.shape
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        # transpose æ˜¯ä¸ºäº†å®ç°çŸ©é˜µä¹˜æ³•
        attn_scores = queries @ keys.transpose(1, 2)
        # å³ä¸Šæ–‡è¯´è¿‡çš„ï¼Œå°†æ©ç ä» 0 ä¿®æ”¹ä¸º -infï¼Œå†è¿›è¡Œé®è”½æ“ä½œ
        attn_scores.masked_fill_(  # New, _ ops are in-place
            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
        # ç»è¿‡ softmax 
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)
        # è¿›è¡Œ dropout
        attn_weights = self.dropout(attn_weights) # New
        # å¾—åˆ°æœ€åç»“æœ
        context_vec = attn_weights @ values
        return context_vec

# å®éªŒä¸€ä¸‹
torch.manual_seed(123)

block_size = batch.shape[1]
ca = CausalAttention(d_in, d_out, block_size, 0.0)

context_vecs = ca(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```



### å¤šå¤´æ³¨æ„åŠ›

- ç›´æ¥æ‹¼æ¥è¾“å‡º 

```
# å®šä¹‰ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚
class MultiHeadAttentionWrapper(nn.Module):

    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):
        super().__init__()
            # å°† num_heads ä¸ªå•å¤´æ³¨æ„åŠ›å±‚ç»„åˆåœ¨ä¸€èµ·æ¥å®ç°å¤šå¤´
        self.heads = nn.ModuleList(
            [CausalAttention(d_in, d_out, block_size, dropout, qkv_bias) 
             for _ in range(num_heads)]
        )

    def forward(self, x):
        # å‰å‘è®¡ç®—æ—¶å°†å¤šä¸ªå¤´çš„è¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·
        return torch.cat([head(x) for head in self.heads], dim=-1)


# å®éªŒä¸€ä¸‹
torch.manual_seed(123)

block_size = batch.shape[1] # token æ•°é‡
d_in, d_out = 3, 2
mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)

context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```



- é€šè¿‡æƒé‡çŸ©é˜µåˆ†å‰²

å¦å¤–ä¸€ç§æ–¹å¼æ˜¯ï¼Œé€šè¿‡å¼ é‡é‡å¡‘QKVçŸ©é˜µï¼Œæ¥è¿›è¡Œè®¡ç®—ã€‚ã€€



```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):
        super().__init__()
        # å› ä¸ºè¦å¯¹æƒé‡çŸ©é˜µæŒ‰æ³¨æ„åŠ›å¤´æ•°è¿›è¡Œæ‹†åˆ†ï¼Œæ‰€æœ‰è¾“å‡ºç»´åº¦å¿…é¡»æ˜¯å¤´æ•°çš„æ•´æ•°å€
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        # head_dim å°±æ˜¯æ‹†åˆ†ä¹‹åæ¯ä¸ªå¤´åº”è¯¥è¾“å‡ºçš„ç»´åº¦
        self.head_dim = d_out // num_heads 

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        # å½¢çŠ¶ä¸º (b, num_tokens, d_out)
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        # æˆ‘ä»¬å¯ä»¥é€šè¿‡å¢åŠ ä¸€ä¸ª num_heads çš„ç»´åº¦æ¥å°†çŸ©é˜µåˆ†å‰²åˆ°æ¯ä¸ªå¤´
        # ç»´åº¦å˜åŒ–: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) 
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # è½¬ç½®ä¸€ä¸‹: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        # åŸºäºçŸ©é˜µä¹˜æ³•ï¼Œç®€å•åœ°å®ç°å„ä¸ªå¤´çš„å¹¶è¡Œè®¡ç®—
        attn_scores = queries @ keys.transpose(2, 3) 
        # ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä¼šå°†æ©ç çŸ©é˜µè½¬åŒ–ä¸º bool å€¼å¹¶åŸºäºåºåˆ—çš„é•¿åº¦è¿›è¡Œæˆªæ–­
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        # éœ€è¦å°†æ©ç çŸ©é˜µ unsqueeze ä¸¤æ¬¡ï¼Œä¹Ÿå°±æ˜¯å¢åŠ ä¸¤ä¸ªç»´åº¦ï¼Œæ‰èƒ½è®©æ©ç çŸ©é˜µçš„ç»´åº¦å’Œæ³¨æ„åŠ›æƒé‡å¯¹åº”ä¸Š
        mask_unsqueezed = mask_bool.unsqueeze(0).unsqueeze(0)
        # ä½¿ç”¨æ©ç çŸ©é˜µæ¥è¿›è¡Œé®è”½
        attn_scores.masked_fill_(mask_unsqueezed, -torch.inf)
        
        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # å½¢çŠ¶: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2) 
        
        # å°†å¤šä¸ªå¤´çš„è¾“å‡ºé‡æ–°ç»„åˆå›å» self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec) # optional projection

        return context_vec

# è¯•éªŒä¸€ä¸‹
torch.manual_seed(123)

batch_size, block_size, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads=2)

context_vecs = mha(batch)

print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```



### æ³¨æ„åŠ›æœºåˆ¶çš„è¿›é˜¶æ€è€ƒ 

**å…ƒ**

æœ¬è´¨ä¸Šæ˜¯XXX

**å**

å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—é‡è¿‡å¤§ã€‚  



**ç©º**

- MHAï¼Œï¼­ï¼±ï¼¡ï¼ŒGQAå¯¹æ¯”:

MHA ï¼Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œtransformerè®ºæ–‡ä¸­æå‡ºã€‚

ï¼­ï¼±ï¼¡ï¼Œ MHA æ”¹è¿›ç‰ˆï¼Œæ¯ä¸ªå¤´æœ‰ä¸€ä¸ªQï¼Œæ‰€æœ‰çš„å¤´å…±äº«KVçŸ©é˜µã€‚

GQAï¼Œã€€ç»¼åˆç‰ˆï¼Œæ¯ä¸ªå¤´æœ‰ä¸€ä¸ªQï¼Œæœ‰å¤šç»„çš„KVã€‚

| æœºåˆ¶ | Query    | Key/Value  | å‚æ•°é‡/é€Ÿåº¦   | è¡¨è¾¾èƒ½åŠ› |
| ---- | -------- | ---------- | ------------- | -------- |
| MHA  | æ¯å¤´ç‹¬ç«‹ | æ¯å¤´ç‹¬ç«‹   | å‚æ•°æœ€å¤š/æ…¢   | æœ€å¼º     |
| MQA  | æ¯å¤´ç‹¬ç«‹ | å…¨éƒ¨å¤´å…±äº« | å‚æ•°æœ€å°‘/æœ€å¿« | è¾ƒå¼±     |
| GQA  | æ¯å¤´ç‹¬ç«‹ | æ¯ç»„å¤´å…±äº« | å‚æ•°é€‚ä¸­/è¾ƒå¿« | é€‚ä¸­     |



- FlashAttentionæ˜¯ä»€ä¹ˆï¼Ÿ

flashAttentionæ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹å¼ã€‚ ä¼ ç»Ÿçš„æ³¨æ„åŠ›è®¡ç®—çš„æ–¹æ³•, éœ€è¦è®¡ç®—QK(T)çŸ©é˜µç›¸ä¹˜ï¼Œæ—¶é—´å¤æ‚åº¦ä¸ºO(n2), å†å¯¹å¤§çŸ©é˜µè¿›è¡Œsoftmax.  flashAttentionçš„æ ¸å¿ƒä¸ºï¼Œå°†å¤§çŸ©é˜µåˆ†æˆblockï¼Œä»¥blockä½œä¸ºæ ¸å¿ƒè®¡ç®—çŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œä»¥åŠå½’ä¸€åŒ–è®¡ç®—ã€‚æœ‰äº†flashAttentionä¹‹åï¼Œå¯ä»¥æé«˜è®¡ç®—çš„æ•ˆç‡ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Q, K, V    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åˆ†å—åŠ è½½å°å— â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è®¡ç®—åˆ†æ•°     â”‚
â”‚ è¾¹softmax    â”‚
â”‚ è¾¹åŠ æƒæ±‚å’Œ   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¤„ç†ä¸‹ä¸€ä¸ªå— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```



- KVcacheçš„åŸç†

å¤§æ¨¡å‹ä¸€æ¬¡åå‡ºä¸€ä¸ªè¯ã€‚åœ¨åŸå§‹çš„transformerä¸­ï¼Œåœ¨æ¨ç†çš„æ—¶å€™ï¼Œéœ€è¦è®¡ç®—é‡æ–°è®¡ç®—å†å²tokençš„KVï¼Œæ–°çš„tokenè¦ç”ŸæˆKVQï¼Œç”¨Qå»æŸ¥è¯¢å†å²tokençš„KVï¼Œæ•æ‰ç‰¹å¾ã€‚ å› æ­¤å†å²çš„KVå®Œå…¨å¯ä»¥å¤ç”¨ ã€‚

kv-cache çš„ shape æ˜¯

[batch_size, n_heads, past_seq_len, head_dim]

past_seq_lenï¼šå†å²tokené•¿åº¦ï¼Œä¼šéšç€ç”Ÿæˆæ­¥æ•°å¢é•¿ã€‚ 

kv-cacheé¢ä¸´å†…å­˜å¢é•¿çš„é—®é¢˜ï¼Œç›®å‰æœ‰çš„åŠæ³•æ˜¯å°†æ¨¡å‹åˆ†åˆ°ä¸åŒçš„GPUä¸Šï¼Œæˆ–è€…é‡‡ç”¨é‡åŒ–çš„æ–¹å¼å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚

| åç§°       | ç»´åº¦ï¼ˆå¤šå¤´ï¼‰                             | ç”¨é€”              |
| ---------- | ---------------------------------------- | ----------------- |
| Q          | [batch, n_heads, seq_len, head_dim]      | å½“å‰tokenæŸ¥è¯¢å‘é‡ |
| K          | [batch, n_heads, seq_len, head_dim]      | å†å²tokené”®å‘é‡   |
| V          | [batch, n_heads, seq_len, head_dim]      | å†å²tokenå€¼å‘é‡   |
| kv-cache K | [batch, n_heads, past_seq_len, head_dim] | ç¼“å­˜å†å²K         |
| kv-cache V | [batch, n_heads, past_seq_len, head_dim] | ç¼“å­˜å†å²V         |





- VLLMçš„åŠ é€ŸåŸç†

pageAttention å’Œ Continuous Batching

ä¼ ç»Ÿçš„kv-cacheåœ¨è¿›è¡Œç®¡ç†çš„æ—¶å€™ï¼Œä¼šåˆ†é…ä¸€ä¸ªå¤§å¼ é‡ï¼Œå¼ é‡çš„sizeä¸º(layer, batch_size, sequence_length, heaer_count,  head_dim)

sequence_length é€šå¸¸ä¼špaddingåˆ°åŒä¸€ä¸ªé•¿åº¦ï¼Œå¯¼è‡´æ˜¾å­˜æµªè´¹ã€‚å¹¶ä¸”åˆ†é…å¤§å¼ é‡ï¼Œä¼šå¯¼è‡´æ˜¾å­˜ç¢ç‰‡ã€‚

ä½ çŒœåˆ°äº†å—ï¼Ÿ pageAttention å’Œå†…å­˜çš„åˆ†é¡µç®¡ç†æ€æƒ³ä¸€æ ·, ä¸å†ç»™è¯·æ±‚åˆ†é…ä¸€ä¸ªå¤§å—æ˜¾å­˜ã€‚è€Œæ˜¯ç»™å…ˆå°†æ˜¾å­˜åˆ†å‰²æˆè¾ƒå°çš„é¡µï¼ŒåŒä¸€ä¸ªè¯·æ±‚å¯ä»¥æ¨ªè·¨ä¸åŒçš„é¡µã€‚å¤šä¸ªè¯·æ±‚çš„kvå¯ä»¥æ‹¼æ¥åœ¨ä¸€ä¸ªå¤§å¼ é‡é‡Œã€‚è¿™æ ·ä¸ä¼šé€ æˆpaddingæµªè´¹ã€‚

å½“è¯·æ±‚è¿‡æ¥ï¼Œé€šè¿‡**é¡µè¡¨**æŸ¥è¯¢å†å²tokenåœ¨é‚£ä¸€é¡µã€‚

pageAttention,  kvè¿›è¡ŒçŸ©é˜µè¿ç®—çš„æ—¶å€™ï¼Œæ˜¯ä¸æ˜¯è¦copyä¸€ä»½? ã€€å¹¶ä¸æ˜¯ã€‚

 åœ¨åšAttentionè®¡ç®—æ—¶ï¼Œ**é€šè¿‡é«˜æ•ˆçš„ç´¢å¼•ï¼ˆgather/scatterï¼‰æ“ä½œï¼ŒæŠŠéœ€è¦çš„kvåœ¨è®¡ç®—æ—¶â€œé€»è¾‘ä¸Šæ‹¼æˆâ€ä¸€ä¸ªè¿ç»­çŸ©é˜µ**

**Continuous Batching**  , èƒ½å¤Ÿå°†ä¸åŒæ—¶é—´å‘èµ·æ¥çš„è¯·æ±‚ï¼Œæ‹¼æ¥åˆ°ä¸€ä¸ªbatché‡Œæ¨ç†,GPUåˆ©ç”¨ç‡æ›´é«˜ï¼Œç›¸åº”æ›´å¿«ã€‚

ä¹Ÿå°±æ˜¯èƒ½åŠ¨æ€çš„æ‰©å±•å½“å‰batch 



### å°ç»“





## å››ã€å®ç°ä¸€ä¸ªGPTæ¨¡å‹

### transformer block



ransformer block æ˜¯å¤§æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ã€‚æ¨¡å‹å‚æ•°ä¸­çš„å±‚æ•°`num_layers`ï¼ŒæŒ‡çš„å°±æ˜¯transformer blockçš„ä¸ªæ•°ã€‚gpt2çš„`num_layers`ä¸º48ï¼Œgpt3ä¸º96.

 å®ƒåŒ…æ‹¬å› æœæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œ, ä»¥åŠåœ¨è¿›å…¥å› æœæ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œå±‚ä¹‹å‰çš„å±‚å½’ä¸€åŒ–ã€‚å¹¶ä¸”é€šè¿‡å¿«æ·è¿æ¥æ¥è¿æ¥å› æœæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç¥ç»ç½‘ç»œå±‚ã€‚ 

å½“è¾“å…¥Xè¿›æ¥ï¼Œtransformer blockå¦‚ä½•è¿è½¬å‘¢ï¼Ÿ  Xä¼šä¾æ¬¡ç»è¿‡å› æœæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œå±‚çš„å¤„ç†ã€‚



é¦–å…ˆï¼ŒXä¼šç»è¿‡è¢«å½’ä¸€åŒ–ï¼Œå¤„ç†æˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„å‘é‡å€¼ Ynormã€‚

æ¥ç€ï¼ŒYnormæ³¨å…¥å› æœæ³¨æ„åŠ›å±‚ï¼Œå¾—åˆ°Yatt, ç„¶åç»è¿‡Dropout å¾—åˆ°Yatt_drop

ç„¶ååº”ç”¨å¿«æ·è¿æ¥ï¼Œå°† X + Yatt_drop å¾—åˆ°çœŸæ­£çš„Yã€‚

 

Yä½œä¸ºæ–°çš„X, ä¼ å…¥åé¦ˆç¥ç»ç½‘ç»œã€‚

åŒæ ·ï¼ŒXä¼šè¢«å½’ä¸€åŒ–å¤„ç†ï¼Œå¤„ç†æˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„å‘é‡å€¼ Ynormã€‚

æ¥ç€ï¼Œç»è¿‡åé¦ˆç¥ç»ç½‘ç»œï¼Œå¾—åˆ°Yffn, å¹¶ç»è¿‡Dropout å¾—åˆ°Yffn_dropã€‚

æœ€åå†æ‹¼æ¥ X å’ŒYffn_drop å¾—åˆ°ï¼¹ã€‚ã€€



todoï¼šä½¿ç”¨å›¾æ¥æè¿°æ›´åŠ æ¸…æ™°  



```
class TransformerBlock(nn.Module):  # éœ€è¦ç»§æ‰¿nn.Module
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiAtention(
            d_in = cfg["emb_dim"],
            d_out = cfg["emb_dim"],
            n_heads = cfg["n_heads"],
            context_length = cfg["context_length"],
            dropout = cfg["drop_rate"],
            qkv_bias = cfg["qkv_bias"]
        )
        self.norm1 = NormLayer(cfg["emb_dim"])
        self.norm2 = NormLayer(cfg["emb_dim"])
        self.drop = nn.Dropout(cfg["drop_rate"])  
        self.ffn = ForwardFeedLayer(cfg["drop_rate"])  
        
    def forward(self, x):
        shortcut = x
        x = self.norm1(x)  
        x = self.att(x)    
        x = self.drop(x)
        x = shortcut + x
        
        shortcut = x
        x = self.norm2(x)  
        x = self.ffn(x)
        x = self.drop(x)
        
        return shortcut + x

		
```



#### å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€€

#### å‰é¦ˆç¥ç»ç½‘ç»œ

- çº¿æ€§å±‚
- æ¿€æ´»å‡½æ•°GELU

å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯ä»€ä¹ˆ?



ç¥ç»ç½‘ç»œä¸€ç§æ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„æ¨¡å‹ã€‚  ç”Ÿç‰©å¤§è„‘ï¼Œæ¥å—å¤–ç•Œçš„è¾“å…¥ï¼Œç»è¿‡ç¥ç»å…ƒçš„å¤„ç†ï¼Œè¾“å‡ºç”µä¿¡å·ï¼Œå†ä¼ é€’ç»™ä¸‹ä¸€ä¸ªç¥ç»å…ƒã€‚ç»è¿‡æ•°åƒä¸‡ä¸ªç¥ç»å…ƒçš„åè°ƒï¼Œæœ€ç»ˆè¾“å‡ºã€‚



ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒã€‚ä½œä¸ºæ•°å­¦æ¨¡å‹ï¼Œç¥ç»å…ƒç”±æƒé‡ã€åç½®ã€æ¿€æ´»å‡½æ•°æ„æˆã€‚

ç»ç”±æƒé‡ã€åç½®ï¼Œå¯¹è¾“å…¥è¿›è¡Œçº¿æ€§å˜æ¢ã€‚

ç»ç”±æ¿€æ´»å‡½æ•°è¿›è¡Œéçº¿æ€§å˜åŒ–åè¾“å‡ºã€‚  



æœ€åŸºç¡€çš„ç¥ç»ç½‘ç»œç”±çº¿æ€§å±‚å’Œæ¿€æ´»å‡½æ•°æ„æˆ ã€‚ çº¿æ€§å±‚æ˜¯åªèƒ½è¿›è¡Œçº¿æ€§å˜æ¢çš„ç»„ä»¶ã€‚æ¿€æ´»å‡½æ•°å¯¹çº¿æ€§å±‚çš„è¾“å‡ºè¿›è¡Œéçº¿æ€§çš„å˜æ¢ã€‚

å¦‚ä¸‹å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å±‚ï¼Œdinä»£è¡¨è¾“å…¥å‚æ•°ç»´åº¦ï¼Œdoutä»£è¡¨è¾“å‡ºçš„ç»´åº¦ã€‚

```ã€€python
layer = torch.nn.Linear(100,200)  #din,dout
```



ä¸¾ä¸ªä¾‹å­ï¼š

ä¸€ä¸ªç”±ä¸¤ä¸ªçº¿æ€§å±‚å’Œæ¿€æ´»å‡½æ•°æ„æˆçš„å‰é¦ˆç¥ç»ç½‘ç»œç¥ç»ç½‘ç»œ

```python
class Feedforward(nn.Module):
   
   def  __init__(self,cfg):
       super().__init__()
       self.layers = nn.Sequential(
       		nn.Linear(cfg["emb_dim"], 4* cfg["emb_dim"]),
            nn.GELU(),
            nn.Linear(4* cfg[ "emb_dim"], cfg["emb_dim"])
       )
    
    def forward(self,x):
        return self.layers(x)
    

```

æ€è€ƒï¼šå‡è®¾ cfg["emb_dim"] = 256, ä¸Šé¢è¿™ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œæœ‰å¤šå°‘ä¸ªå‚æ•°ï¼Ÿå‚æ•°ä¸­çš„æƒé‡çŸ©é˜µå‚æ•°å’Œåç½®é¡¹åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ

æé†’ï¼šåç½®é¡¹ä¸ªæ•°ç­‰äºè¾“å‡ºç»´åº¦ã€‚

è®©æˆ‘ä»¬æ¥å®ç°GPTä¸­çš„å‰é¦ˆç¥ç»ç½‘ç»œã€€



GELUå‡½æ•°

```
class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) * 
            (x + 0.044715 * torch.pow(x, 3))
        ))
```



å®Œæ•´çš„å‰é¦ˆç¥ç»ç½‘ç»œ

```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
            nn.Dropout(cfg["drop_rate"])
        )

    def forward(self, x):
        return self.layers(x)
```



#### å±‚å½’ä¸€åŒ–



å±‚å½’ä¸€åŒ–æ˜¯ä¸€ç§å¢åŠ æ¨¡å‹è®­ç»ƒç¨³å®šæ€§çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œ èƒ½é¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦çˆ†ç‚¸ã€‚

å±‚å½’ä¸€åŒ–çš„æµç¨‹ä¸ºï¼Œè°ƒæ•´ç¥ç»ç½‘ç»œå±‚çš„è¾“å‡ºï¼Œè®©å®ƒç¬¦åˆï¼šâ€å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1â€œçš„è§„åˆ™ã€‚

å†å°†å±‚å½’ä¸€åŒ–ä¹‹åçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚çš„è¾“å…¥ã€‚

å…·ä½“æ­¥éª¤ä¸ºï¼š

- æ±‚è¾“å‡ºçš„å‡å€¼å’Œæ–¹å·®
- è¾“å‡ºä¸­å„ä¸ªç»´åº¦çš„å€¼å‡å»å‡å€¼ ï¼› é™¤ä»¥æ–¹å·®çš„å¹³æ–¹æ ¹

åœ¨transformer blockä¸­ï¼Œä¸¤æ¬¡è¿ç”¨äº†å±‚å½’ä¸€åŒ–ã€‚åˆ†åˆ«åœ¨å› æœæ³¨æ„åŠ›æœºåˆ¶è¾“å…¥ä¹‹å‰ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œè¾“å…¥ä¹‹å‰ã€‚ 



ä¸‹é¢å±‚å½’ä¸€åŒ–å±‚çš„å®ç°

```
 class Normlayer(nn.Module):
 	def __init__(self,emb_dim):
 		super().__init__()
 		self.eps = 1e-5
 		self.scale = torch.nn.Parameter(torch.ones(emb_dim))
 		self.shift = torch.nn.Parameter(torch.zeros(emb_dim))
 	def forward(self,x):
		mean = x.mean(-1,keepdim= True)
		var  = x.var(-1,keepdim = True,unbiased=False)
		x = (x -mean)/torch.sqrt(var+ self.eps)
		return  self.scale *x + self.shift
 	
```

unbiased=False çš„æ„æ€æ˜¯ä½¿ç”¨æœ‰åæ–¹å·®ï¼ŒåŸå› åœ¨äºæ•°æ®é‡æ¯”è¾ƒå¤§ï¼Œè¯¯å·®å¯ä»¥å¿½ç•¥ä¸è®°ã€‚

#### æ®‹å·®è¿æ¥

å¿«æ·è¿æ¥(shortcut connectionï¼Œåˆå«æ®‹å·®è¿æ¥)æ˜¯ä»€ä¹ˆï¼Ÿ

å¿«æ·è¿æ¥ä¸€ç§åœ¨ä¸åŒå±‚ä¹‹é—´å¢åŠ è¿æ¥çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚å¿«æ·è§£å†³äº†åå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦è¿‡å°çš„é—®é¢˜ â€”â€”è¶Šé å‰çš„ç¥ç»ç½‘ç»œå±‚æ¢¯åº¦è¶Šå°ã€‚

å¦‚ä½•å®ç°å¿«æ·è¿æ¥ï¼Ÿ 

å°†å½“å‰ç¥ç»ç½‘ç»œçš„è¾“å…¥ x, æ·»åŠ åˆ°è¾“å‡ºä¹‹ä¸­ã€‚ä¹Ÿå°±æ˜¯x + è¾“å‡º = æ–°çš„è¾“å‡ºã€‚

å½“ä½ åœ¨æ„å»ºä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œæ—¶ï¼Œå¯ä»¥åœ¨ä¸åŒçš„ç¥ç»ç½‘ç»œå±‚ä¹‹é—´æ·»åŠ å¿«æ·è¿æ¥ï¼Œä»è€Œé¿å…æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚

åœ¨shortcut connectä¹‹å‰ï¼Œå¯ä»¥æ‰§è¡Œdropoutï¼Œè¿™ä¹Ÿæ˜¯transformer blockä¸­çš„åšæ³•ã€‚



#### tranformer blockä»£ç å®ç° 



```python
from previous_chapters import MultiHeadAttention

class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            block_size=cfg["ctx_len"],
            num_heads=cfg["n_heads"], 
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # æ³¨æ„åŠ›å—ä¸­çš„Shortcutè¿æ¥
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # ä¸åŸå§‹è¾“å…¥å—æ±‚å’Œ

        # å‰é¦ˆå—ä¸­çš„Shortcutè¿æ¥
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # ä¸åŸå§‹è¾“å…¥å—æ±‚å’Œ

        return x
```



### GPTç±»å®ç°ï¼ˆæ•°æ®è¾“å…¥ï¼Œè¾“å‡ºï¼‰



#### GPTæ ¸å¿ƒæœ‰å“ªäº›ç»„ä»¶ï¼Ÿ

```
class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["ctx_len"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        
        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])
        
        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(
            cfg["emb_dim"], cfg["vocab_size"], bias=False
        )

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
```



logitsçš„ å½¢çŠ¶: (batch_size, seq_length, vocab_size)



forwardå‡½æ•°

- embedding + pos- embedding 
- Nä¸ª transformer
- final-norm
- head-out



å¦‚ä½•è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡ 

```
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params:,}")
```



### ç®€å•æ–‡æœ¬ç”Ÿæˆ

ä½¿ç”¨è´ªå©ªè§£ç ç”Ÿæˆæ–‡æœ¬ã€‚

```python
def generate_text_simple(model, idx, max_new_tokens, context_size):
    # idxæ˜¯å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„ç´¢å¼•æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(B, T)
    for _ in range(max_new_tokens):

        # å¦‚æœå½“å‰ä¸Šä¸‹æ–‡è¶…è¿‡äº†æ”¯æŒçš„é•¿åº¦ï¼Œå°±å¯¹å½“å‰ä¸Šä¸‹æ–‡è¿›è¡Œæˆªæ–­
        # ä¾‹å¦‚ï¼Œå¦‚æœLLMåªæ”¯æŒ5ä¸ªtokenï¼Œè€Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º10ï¼Œ
        # é‚£ä¹ˆåªæœ‰æœ€å5ä¸ªtokenä¼šè¢«ç”¨ä½œä¸Šä¸‹æ–‡

        idx_cond = idx[:, -context_size:]
        
        # è·å–é¢„æµ‹ç»“æœ
        with torch.no_grad():
            logits = model(idx_cond)
        
        # åªå…³æ³¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        # (batch, n_token, vocab_size)å˜ä¸º(batch, vocab_size)
        logits = logits[:, -1, :]  

        # é€šè¿‡softmaxå‡½æ•°è·å¾—å¯¹åº”çš„æ¦‚ç‡
        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)

        # è·å–æ¦‚ç‡å€¼æœ€é«˜çš„å•è¯ç´¢å¼•
        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)

        # å°†é‡‡æ ·åˆ°çš„ç´¢å¼•æ·»åŠ åˆ°å½“å‰è¿è¡Œçš„ä¸Šä¸‹æ–‡ç´¢å¼•åºåˆ—ä¸­
        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)

    return idx
```







æ¨¡å‹æ˜¯å¦‚ä½•ç»“æŸè¾“å‡ºçš„ï¼Ÿ 

â€‹     æ¨¡å‹ä»€ä¹ˆæ—¶å€™ç»ˆæ­¢ï¼Œå—åˆ°ä¸‰ä¸ªå› ç´ çš„å½±å“ã€‚max_sequence , max_new_token, æ¨¡å‹ä¸»åŠ¨è¾“å‡ºåœæ­¢ç¬¦ `eos` .

åœ¨æ¨ç†ä¸­ï¼Œå¦‚æœåºåˆ—çš„é•¿åº¦è¾¾åˆ°äº†max_sequenceï¼Œ æ¨¡å‹å°±ä¸ä¼šå†è¾“å‡ºã€‚å…·ä½“æ¥è¯´ï¼Œ

**ï¼ˆ1ï¼‰å¦‚æœè¾¾åˆ°max_sequence_lengthï¼ˆæœ€å¤§åºåˆ—é•¿åº¦ï¼‰**

- è¿™æ¡åºåˆ—**ä¸èƒ½å†ç”Ÿæˆæ–°çš„token**ï¼Œç›´æ¥åœæ­¢ç”Ÿæˆã€‚
- åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™æ¡åºåˆ—ä¼šè¢«**mask/pad**ï¼Œå³åç»­ç”Ÿæˆæ­¥éª¤ä¸­ï¼Œè¿™æ¡åºåˆ—ä¸å†è¢«â€œæ¿€æ´»â€ï¼Œåªä¿ç•™å·²ç”Ÿæˆå†…å®¹ã€‚

**ï¼ˆ2ï¼‰å¦‚æœæå‰é‡åˆ°ç»ˆæ­¢ç¬¦ï¼ˆå¦‚EOSï¼‰**

- è¿™æ¡åºåˆ—ä¹Ÿä¼šåœæ­¢ç”Ÿæˆï¼Œåç»­æ­¥éª¤ç”¨paddingå¡«å……ã€‚







### GPTæ¶æ„çš„è¿›é˜¶æ€è€ƒ 

**å…ƒ**

**å**



Logitsæ¯æ¬¡éƒ½éœ€è¦è®¡ç®—æ‰€æœ‰ï¼Œå¯¼è‡´ä¼šåˆå¾ˆå¤šæµªè´¹ã€‚å› æ­¤ kv -cacheè¯ç”Ÿäº†

**ç©º**



**ä»€ä¹ˆæ˜¯MOEæ¨¡å‹ï¼Ÿ**
MOEï¼Œæ··åˆä¸“å®¶æ¨¡å‹ï¼Œæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ç”±é—¨æ§å’Œå¤šä¸ªä¸“å®¶ç½‘ç»œæ„æˆã€‚ç”¨äºæ›¿æ¢å‰é¦ˆç¥ç»ç½‘ç»œã€‚å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯å…¨éƒ¨å‚æ•°æ¿€æ´»çš„ï¼Œä¹Ÿå«åšdenseæ¨¡å‹ã€‚è€ŒMOEåˆ™æ˜¯å…ˆç”±é—¨æ§æ¥æ§åˆ¶æ¿€æ´»å“ªäº›ä¸“å®¶ï¼Œå¯ä»¥å‡å°‘æ¨ç†æ—¶çš„å‚æ•°æ¿€æ´»é‡ã€‚ä½†MOEæ¨¡å‹åœ¨è®­ç»ƒéš¾åº¦ä¸Šæ¯”Denseæ¨¡å‹è¦é«˜ã€‚ 



**RMSï¼®ormæ˜¯ä»€ä¹ˆï¼Ÿ** 

RMSNormæ˜¯ä¸€ç§åŸºäºå‡æ–¹æ ¹çš„å½’ä¸€åŒ–æ–¹æ³•ã€‚GPTæœ€å¼€å§‹ä½¿ç”¨çš„LayerNorméœ€è¦å…ˆå‡å»å‡å€¼ï¼Œå†é™¤ä»¥æ–¹å·®ï¼Œä½¿å¾—å‘é‡å˜æˆå‡å€¼ä¸ºï¼ï¼Œæ–¹å·®ä¸ºï¼‘çš„å‘é‡ã€‚RMSNormä¸éœ€è¦è®¡ç®—å‡å€¼ï¼Œä¹Ÿä¸ä½¿ç”¨æ–¹å·®æ¥å½’ä¸€åŒ–ã€‚è€Œæ˜¯å…ˆå°†è®¡ç®—å„é¡¹çš„å¹³æ–¹å’Œçš„å‡å€¼ï¼Œå¼€æ ¹å·ï¼Œå¾—åˆ°å‡æ–¹æ ¹ã€‚å†ç”¨å„é¡¹é™¤ä»¥å‡æ–¹æ ¹ï¼Œå¾—åˆ°å½’ä¸€åŒ–åçš„å€¼ã€‚

RMSï¼®ormçš„è®¡ç®—é‡æ¯”å¸¸è§„çš„LayerNormè¦å°‘ï¼Œé€æ¸è¢«ä¸»æµçš„å¤§æ¨¡å‹é‡‡ç”¨ï¼Œæ¯”å¦‚Qwen





### å°ç»“



## äº”ã€å¦‚ä½•è®­ç»ƒæ¨¡å‹



tokençš„ç¼–ç å’Œè§£ç ã€‚

```python
import tiktoken
from previous_chapters import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # å¢åŠ batchç»´åº¦
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0) # å»æ‰batchç»´åº¦
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["ctx_len"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```





### å¦‚ä½•è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºï¼Ÿ 



ä½¿ç”¨è¾“å‡ºç»“æœå’Œç›®æ ‡çš„è·ç¦»æ¥è¡¡é‡ã€‚  





#### äº¤å‰ç†µ



å¦‚ä½•è®¡ç®—äº¤å‰ç†µï¼Ÿ  

- ç›®æ ‡è¯å…ƒçš„æ¦‚ç‡

  - logits
  - target-token

  ```
  inputs = torch.tensor([[16833, 3626, 6100],   # ["every effort moves",
                         [40,    1107, 588]])   #  "I really like"]
  
  targets = torch.tensor([[3626, 6100, 345  ],  # [" effort moves you",
                          [588,  428,  11311]]) #  " really like chocolate"]
  ```

  

  ```
  with torch.no_grad():
      logits = model(inputs)
  
  probas = torch.softmax(logits, dim=-1) # è¯è¡¨ä¸­æ¯ä¸ªæ ‡è®°çš„é¢„æµ‹æ¦‚ç‡
  print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)
  ```

  

  ```
  batch_idx = 0
  target_probas_1 = probas[batch_idx, [0, 1, 2], targets[batch_idx]]
  print("Batch 1:", target_probas_1)
  
  batch_idx = 1
  target_probas_2 = probas[1, [0, 1, 2], targets[1]]
  print("Batch 2:", target_probas_2)
  ```

  

- å–å¯¹æ•° â€” ä¸ºä»€ä¹ˆå¯¹æ•°æ›´å®¹æ˜“ä¼˜åŒ–ï¼Ÿ

```
# è®¡ç®—æ‰€æœ‰æ ‡è®°çš„é¢„æµ‹æ¦‚ç‡çš„å¯¹æ•°å€¼
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)
```

- è®¡ç®—å¹³å‡æ•° 

```
# å¯¹æ‰€æœ‰æ ‡è®°çš„æ¦‚ç‡å¯¹æ•°å€¼æ±‚å‡å€¼
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)
```

- å–è´Ÿæ•° â€” æ·±åº¦å­¦ä¹ ä¹‹ä¸­ç»å¸¸ä½¿ç”¨çš„æ˜¯å‡å°‘åˆ°0ï¼Œè€Œä¸æ˜¯å¢åŠ åˆ°0

```
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)
```



ä½¿ç”¨pytorchä¸­çš„entropy_losså‡½æ•°ï¼Œå¯ä»¥è¿›è¡Œè®¡ç®—ã€‚

- å…ˆåœ¨batchç»´åº¦ä¸Šå±•å¹³è¿™äº›å‘é‡ 

  ```
  logits_flat = logits.flatten(0, 1)
  targets_flat = targets.flatten()
  
  print("Flattened logits:", logits_flat.shape)
  print("Flattened targets:", targets_flat.shape) 
  ```

  

- ä½¿ç”¨äº¤å‰ç†µå‡½æ•°

```
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)
```

#### å›°æƒ‘åº¦

å›°æƒ‘åº¦æ˜¯ä»€ä¹ˆï¼Ÿ 

å›°æƒ‘åº¦æ˜¯å¯¹äº¤å‰ç†µè¿›è¡ŒæŒ‡æ•°è®¡ç®—çš„ç»“æœã€‚ å›°æƒ‘åº¦æ›´æœ‰è§£é‡Šæ€§ï¼Œæ„å‘³ç€æ¨¡å‹åœ¨ä¸‹ä¸€æ­¥ä¸­æ‰€ä¸ç¡®å®šçš„è¯è¡¨çš„å¤§å°ã€‚

æ¯”å¦‚ï¼Œå½“å›°æƒ‘åº¦ä¸º10ï¼Œé‚£ä¹ˆæ„å‘³ç€ä¸‹ä¸€ä¸ªè¯ä¸ç¡®å®šæ˜¯10ä¸­çš„å“ªä¸€ä¸ªã€‚



#### è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æŸå¤±



```
from previous_chapters import create_dataloader_v1

# è®­ç»ƒé›†/éªŒè¯é›†æ•°æ®æ¯”
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]


torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["ctx_len"],
    stride=GPT_CONFIG_124M["ctx_len"],
    drop_last=True,
    shuffle=True
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["ctx_len"],
    stride=GPT_CONFIG_124M["ctx_len"],
    drop_last=False,
    shuffle=False
)
```



### æ¨¡å‹è®­ç»ƒçš„åŸºæœ¬æµç¨‹







### é«˜é˜¶çš„è®­ç»ƒæŠ€å·§



- æ··åˆç²¾åº¦è®­ç»ƒ

æ··åˆç²¾åº¦æ˜¯æŒ‡ï¼Œ ä½¿ç”¨`float32`æ¥å­˜å‚¨æƒå’Œæ›´æ–°å‚æ•°ï¼Œåœ¨å‘å‰ä¼ æ’­ï¼Œåå‘ä¼ æ’­çš„æ—¶å€™ï¼Œéƒ½ä½¿ç”¨`float16`æˆ–è€…`bfloat16`.

åœ¨è®¡ç®—å®Œæ¢¯åº¦ä¹‹åï¼Œè®²æ›´æ–°çš„æ¢¯åº¦è½¬åŒ–å›`float32`, å†è¿›è¡Œæƒé‡å‚æ•°æ›´æ–°ã€‚

å› ä¸º`float16`è¡¨ç¤ºçš„èŒƒå›´æœ‰é™ï¼Œå½“å°äºæŸä¸ªå€¼çš„æ—¶ï¼Œä¼šç›´æ¥å˜æˆ0ï¼Œè¿™ä¸ªç°è±¡å«åšã€Œä¸‹æº¢ã€ã€‚è§£å†³çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨`bfloat16`æˆ–è€…æŸå¤±ç¼©æ”¾ã€‚`bfloat16`æ‰€èƒ½è¡¨ç¤ºçš„èŒƒå›´å’Œ`float32`å·®ä¸å¤šï¼Œä½†ç²¾åº¦æ¯”`float16`è¦å·® ã€‚

æŸå¤±ç¼©æ”¾ï¼Œè®²æŸå¤±å‡½æ•°æ”¾å¤§ï¼Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œæ¢¯åº¦ä¹Ÿæ”¾å¤§äº†ï¼Œé¿å…äº†ã€Œä¸‹æº¢ã€ã€‚ æƒé‡æ›´æ–°çš„æ—¶å€™ï¼Œåœ¨æŒ‰æ¯”ä¾‹æ¥ç¼©å°ã€‚





### è§£ç ç­–ç•¥



å¤§æ¨¡å‹ä¸­çš„æ¸©åº¦é‡‡æ ·å’Œtop-kæ˜¯ä»€ä¹ˆï¼Ÿ

æ¸©åº¦é‡‡æ ·æ˜¯ä¸€ç§ç”¨äºå¤§æ¨¡å‹è§£ç é˜¶æ®µæ¦‚ç‡åŒ–çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæé«˜ç”Ÿæˆtokençš„å¤šæ ·æ€§ã€‚

å¤§æ¨¡å‹æ˜¯é€šè¿‡è®¡ç®—è¯è¡¨ä¸­ï¼Œæ¯ä¸€ä¸ªè¯çš„æ¦‚ç‡æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªç”Ÿæˆçš„è¯ã€‚

è¯è¡¨ä¸­çš„æ¯ä¸ªè¯ï¼Œéƒ½ä¼šå¯¹åº”ä¸€ä¸ªæ¦‚ç‡å€¼ã€‚

æ¯æ¬¡é€‰å–æ¦‚ç‡å€¼æœ€é«˜çš„è¯ï¼Œè¢«ç§°ä¸ºè´ªå©ªè§£ç ã€‚è´ªå©ªè§£ç è¾“å‡ºçš„è¯æ¯”è¾ƒå•ä¸€ã€‚

æ¸©åº¦é‡‡æ ·ï¼Œåˆ™æ˜¯æŒ‰ç…§æ¦‚ç‡å€¼çš„å¤§å°ï¼Œä»ä¸­è¿›è¡ŒéšæœºåŒ–æŠ½å–ï¼Œé€‰å–å‡ºä¸‹ä¸€ä¸ª è¯ã€‚



å‡è®¾è¯è¡¨çš„å¤§å°ä¸º3ï¼Œæ¨¡å‹è¾“å‡ºå•è¯çš„æ¦‚ç‡ä¸ºï¼š0.6,0.2,0.2ã€‚

å¯¹äºç›¸åŒçš„è¾“å…¥:

å¦‚æœæ—¶è´ªå©ªè§£ç ï¼Œæ¯æ¬¡éƒ½ä¼šé€‰å–æ¦‚ç‡å€¼æœ€å¤§0.6çš„è¯è¾“å‡ºã€‚

å¦‚æœæ˜¯æ¦‚ç‡åŒ–çš„æ–¹å¼ï¼ŒæŠ½ä¸­0.6çš„æ¦‚ç‡æœ€é«˜ï¼Œä½†ä¹Ÿä¼šéšæœºæŠ½åˆ°0.2çš„ã€‚



æ¸©åº¦é‡‡æ ·ï¼Œæ˜¯å¯¹æ¦‚ç‡çš„åˆ†å¸ƒç¼©æ”¾å¤„ç†ã€‚

è¿˜æ˜¯ä»¥ï¼ˆ0.6,0.2,0.2ï¼‰ä¸ºä¾‹ã€‚

æ¯ä¸ªæ¦‚ç‡éƒ½ä¼šé™¤ä»¥æ¸©åº¦é‡‡æ ·ã€‚

å½“temp = 1ï¼Œæ¦‚ç‡åˆ†å¸ƒä¸å˜ã€‚

å½“temp < 1,  æ¦‚ç‡åˆ†å¸ƒä¼šé”åŒ–ã€‚å¤§çš„æ¦‚ç‡å€¼ä¼šæ›´åŠ å‡¸æ˜¾ã€‚

å½“temp > 1, æ¦‚ç‡åˆ†å¸ƒä¼šæ‰å¹³åŒ–ã€‚å„ä¸ªæ¦‚å¿µä¹‹é—´çš„å·®è·ä¼šç¼©å°ã€‚

é€šè¿‡è®¾ç½®æ¸©åº¦é‡‡æ ·å€¼ï¼Œæ¥è°ƒæ•´æ¨¡å‹è¾“å‡ºçš„ä¸°å¯Œåº¦ã€‚å½“æ¨¡å‹è¾“å‡ºå¤ªå‘†æ¿ï¼Œå¯ä»¥å°†æ¸©åº¦é‡‡æ ·è°ƒå¤§ï¼Œå°±ä¼šç”Ÿæˆæ›´åŠ ä¸°å¯Œçš„è¯ã€‚



top-kåˆ™å†³å®šå“ªäº›è¯ä¼šè¿›è¡ŒéšæœºæŠ½å–çš„è¿‡ç¨‹ã€‚

æ¯”å¦‚top-k = 30ï¼Œé‚£ä¹ˆåªä¼šä»æ¦‚ç‡æœ€å¤§çš„å‰30ä¸ªè¯ä¸­ï¼ŒéšæœºæŠ½å–ã€‚

top-kèƒ½å¤Ÿå‰”é™¤æ¦‚ç‡å€¼æå°çš„å€¼ï¼Œé˜²æ­¢è¾“å‡ºä¸­å‡ºç°æ— å…³çš„è¯ã€‚





### æ¨¡å‹è®­ç»ƒçš„çš„è¿›é˜¶æ€è€ƒ 



**å…ƒ**



**å**



**ç©º**

ï¼ã€€æ¢¯åº¦è£å‰ªæ˜¯ä»€ä¹ˆï¼Ÿ

æ¢¯åº¦è£å‰ªæ˜¯ä¸€ç§é™åˆ¶æ¢¯åº¦çˆ†ç‚¸çš„æ–¹æ³•ï¼Œä¸€èˆ¬é‡‡ç”¨L2èŒƒæ•°æ¥è¿›è¡Œé™åˆ¶ã€‚åœ¨åå‘ä¼ æ’­ä¹‹åï¼Œå¾—åˆ°æ¢¯åº¦ã€‚

è®¡ç®—æ¢¯åº¦çš„L2èŒƒæ•°ï¼Œå¦‚æœå¤§äºæŸä¸ªé˜ˆå€¼ã€‚å°±å¯¹æ¢¯åº¦è¿›è¡Œç¼©æ”¾ï¼Œå›åˆ°é˜ˆå€¼ä¹‹å†…ã€‚

L1å’ŒL2æ˜¯ä»€ä¹ˆï¼Ÿ

ä¸¤ç§å¸¸è§çš„èŒƒæ•°ï¼Œç”¨æ¥è¡¡é‡å‘é‡çš„å¤§å°ã€‚L1èŒƒæ•°ï¼Œæ›¼å“ˆé¡¿è·ç¦»ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰å…ƒç´ çš„ç»å¯¹å€¼ä¹‹å’Œã€‚L2èŒƒæ•°ï¼Œæ‰€æœ‰æ•°å¹³æ–¹ï¼Œç´¯ç§¯èµ·æ¥ï¼Œå†è¿›è¡Œå¼€æ ¹å·ã€‚

- **L1æ­£åˆ™åŒ–**ï¼šé¼“åŠ±å‚æ•°å˜æˆ0ï¼Œäº§ç”Ÿç¨€ç–æ€§ï¼ˆæœ‰åˆ©äºç‰¹å¾é€‰æ‹©ï¼‰ã€‚
- **L2æ­£åˆ™åŒ–**ï¼šé¼“åŠ±å‚æ•°å˜å°ä½†ä¸ä¸º0ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè®©æ¨¡å‹æ›´å¹³æ»‘ã€‚

- **L1**å°±åƒä½ åœ¨åŸå¸‚è¡—é“é‡Œèµ°è·¯ï¼ˆåªèƒ½æ¨ªç€ç«–ç€ï¼‰ï¼Œèµ°åˆ°ç›®çš„åœ°çš„æ€»æ­¥æ•°ã€‚
- **L2**å°±åƒä½ ç›´æ¥èµ°ç›´çº¿åˆ°ç›®çš„åœ°çš„è·ç¦»ã€‚



### å°ç»“



## å…­ã€å¦‚ä½•ä½¿ç”¨LLamaFactoryå¾®è°ƒæ¨¡å‹





### LLamaFactoryçš„å¾®è°ƒæµç¨‹  





#### æ•°æ®é›†æ„å»º



#### å‚æ•°è®¾ç½®



#### å¼€å§‹è®­ç»ƒ 





### Loraå¾®è°ƒ 



Loraå¾®è°ƒæ˜¯ä»€ä¹ˆï¼Ÿ 



Loraå¾®è°ƒ(Low-Rank Adaptation)ï¼Œæ˜¯ä¸€ç§é«˜æ•ˆç‡çš„å¾®è°ƒæŠ€æœ¯ã€‚å¯ä»¥é™ä½å¾®è°ƒå‚æ•°çš„æ•°é‡ï¼Œä»è€Œé™ä½å¾®è°ƒæ‰€éœ€è¦çš„èµ„æºã€‚



Loraçš„è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2106.09685



Loraå¾®è°ƒä¸ºä»€ä¹ˆé«˜æ•ˆï¼Ÿ 

è®©æˆ‘ä»¬æ¥çœ‹ä¸‹Loraçš„åŸç†ã€‚

å¯¹äºå…¨å‚å¾®è°ƒçš„è¿‡ç¨‹ï¼Œå¯ä»¥ç®€å•è¡¨ç¤º

W1= W  + â–³W

 Wæ˜¯å¾®è°ƒå‰çš„æƒé‡å‚æ•°ï¼Œâ–³Wæ˜¯æƒé‡çš„æ›´æ–°ï¼ŒW  + â–³Wå¾—åˆ°æ–°çš„æƒé‡å‚æ•°Wï¼‘ã€‚éœ€è¦æ³¨æ„ï¼Œâ–³Wåœ¨å¾®è°ƒçš„è¿‡ç¨‹ä¸­æ›´æ–°ï¼Œå’ŒWå…·æœ‰ä¸€æ ·çš„ç»´åº¦å¤§å°ã€‚

Low-Rank Adaptationï¼ŒRankæ˜¯çŸ©é˜µçš„ç§©ï¼Œç§©æ˜¯çŸ©é˜µä¸­çº¿æ€§æ— å…³çš„å‘é‡å€¼ï¼Œä¸ä¼šè¶…è¿‡æœ€å°çš„ç»´åº¦å€¼ã€‚æ¯”å¦‚ä¸€ä¸ª 30* 2çš„çŸ©é˜µï¼Œrank æœ€å¤§ä¸º2.

é™ä½ç§©ï¼ŒæŸç§ç¨‹åº¦ä¸Šå¯ä»¥å‡å°‘è®­ç»ƒæƒé‡ã€‚

Loraåšäº†ä¸€ä¸ªè¿™æ ·çš„äº‹æƒ…ã€‚å°† â–³W è¿‘ä¼¼æˆ AB, ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜.

W1â‰ˆ  W   +  AB

Aå’ŒBæ˜¯ä¸€ä¸ªç§©ç›¸å¯¹å¾ˆå°çš„çŸ©é˜µ, ä¸€èˆ¬è®¾ç½®ä¸º16.

å‘ç°äº†å—ï¼Ÿ å°†ABæ›¿æ¢ â–³Wä¹‹å,éœ€è¦è®­ç»ƒçš„å‚æ•°å°±å¤§å¤§å‡å°‘.  



ä»å·¥ç¨‹å®è·µçš„è§’åº¦æ¥çœ‹,  Loraå‚æ•°å¯ä»¥å•ç‹¬è®­ç»ƒ, å•ç‹¬ä¿å­˜, æ¨ç†çš„æ—¶å€™å†å’ŒåŸå§‹çš„æƒé‡ä¸€èµ·åŠ è½½. 

è¿™ä¸ªå¾—ç›ŠäºçŸ©é˜µä¹˜æ³•ä¹˜æ³•çš„åˆ†é…å¾‹,   

XW1  = XW +  XAB

Xä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ±‚è§£è¾“å‡ºçš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†å¼€è®¡ç®—åŸæ¨¡å‹çš„è¾“å‡ºXW å’ŒLoraæ¨¡å‹çš„è¾“å‡ºXABï¼Œå†æŠŠä»–ä»¬æ‹¼æ¥èµ·æ¥ã€‚



å¦‚ä½•åœ¨ç å±‚é¢å®ç°Loraï¼Ÿ 

é¦–å…ˆï¼Œæˆ‘ä»¬è¦å®šä¹‰ä¸€ä¸ªç±»LoraLayer,  ç»§æ‰¿ Moduleã€‚ ä¼ å…¥å‚æ•°åŒ…æ‹¬d_in, d_out, rankï¼Œä»¥åŠä»¥åŠç¼©æ”¾å‚æ•°alphaã€‚ 

åŒ…å«ä¸¤ä¸ªæƒé‡çŸ©é˜µ:Aå’ŒB, Aåˆå§‹åŒ–ï¼ŒBå…¨ä¸º0.

forwadå‡½æ•°å®šä¹‰ä¸º ç¼©æ”¾alpha*  ï¼ˆ(X @B @C)ã€‚

```py
import math

class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha
        
    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x

```

`ä»£ç ä¸­ä½¿ç”¨äº†torch.nn.init.kaiming_uniform_` ,æ˜¯ä¸€ä¸­å¯¹æƒé‡å‚æ•°åˆå§‹åŒ–çš„æ–¹æ³•ï¼Œå®ƒèƒ½ç¡®ä¿Aåœ¨å¼€å§‹è®­ç»ƒæ—¶ï¼Œä¸ä¼šçªç„¶æ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ï¼Œæœ‰è‰¯å¥½çš„æ”¶æ•›æ•ˆæœã€‚



åœ¨æœ‰äº†åŸºæœ¬çš„Loraç±»ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸Šé¢æåˆ°çš„æ‹¼æ¥è¾“å‡ºçš„åŠŸèƒ½ã€‚ä¹Ÿå°±æ˜¯æŠŠå¸¸è§„çš„çº¿æ€§å±‚å’ŒLoraç±»ç»„åˆèµ·æ¥å½¢æˆæ–°çš„ç±»ï¼Œforwadå‡½æ•°ä¸ºä¸¤ä¸ªçš„æ‹¼æ¥è¾“å‡ºã€‚

```py
class LinearWithLoRA(torch.nn.Module):
	def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
        linear.in_features,
        linear.out_features,
        rank,
        alpha
)
    def forward(self, x):
    	return self.linear(x) + self.lora(x)
```

å†å°±æ˜¯éœ€è¦å®ç°ä¸€ä¸ªå‡½æ•°ï¼ŒæŠŠæ¨¡å‹ä¸­çš„çº¿æ€§å±‚ï¼Œéƒ½æ›¿æ¢æˆå¾…æœ‰Loraçš„çº¿æ€§å±‚ã€‚

```py
def replace_linear_with_lora(model, rank, alpha):
    for name, module in model.named_children():
        if isinstance(module, torch.nn.Linear):
            # å¦‚æœæ˜¯çº¿æ€§å±‚ï¼Œæ›¿æ¢ä¸º LoRA ç‰ˆæœ¬
            setattr(model, name, LinearWithLoRA(module, rank, alpha))
        else:
            # å¦‚æœä¸æ˜¯çº¿æ€§å±‚ï¼Œé€’å½’å¤„ç†å…¶å­æ¨¡å—
            replace_linear_with_lora(module, rank, alpha)
```

å…¶ä¸­ `model.named_children()` æ˜¯ `nn.Module` ä¸­è¿”å›å­æ¨¡å—çš„æ–¹æ³•ï¼Œè¿”å›å¯¹è±¡æ˜¯ä¸€ä¸ªå…ƒç»„ã€‚ä»…åŒ…å«å­æ¨¡å—ï¼Œä¸åŒ…å«å­æ¨¡å—çš„å­æ¨¡å—ï¼Œæ‰€ä»¥ä¸‹é¢è¦é‡‡ç”¨é€’å½’è°ƒç”¨çš„æ–¹å¼ã€‚

id: 20250511210355
ref:ã€Šä»é›¶æ„å»ºå¤§æ¨¡å‹ã€‹
changeLog: 

- 20250511 Init



### æ•°æ®å¾®è°ƒçš„è¿›é˜¶æ€è€ƒ 

### å°ç»“



## ä¸ƒã€é™„å½•



## å¼ é‡çš„åŸºæœ¬æ“ä½œ

æ·±åº¦å­¦ä¹ ä¸­çš„è¾“å‡ºå‚æ•°å’Œè¾“å‡ºå‚æ•°æœ¬è´¨ä¸Šéƒ½æ˜¯å¼ é‡ã€‚é€šè¿‡äº†è§£å¼ é‡çš„å˜åŒ–ï¼Œäº†è§£æ¨¡å‹è¿›è¡Œä½•ç§è½¬åŒ–ã€‚

- åŸºæœ¬å±æ€§

```
x = torch.randn(3, 4, 5)

# å½¢çŠ¶
print(x.shape)  # torch.Size([3, 4, 5])
print(x.size())  # åŒä¸Š

# ç»´åº¦æ•°é‡
print(x.dim())  # 3

# æ•°æ®ç±»å‹
print(x.dtype)  # torch.float32

# è®¾å¤‡ä½ç½®
print(x.device)  # cpu æˆ– cuda:0

# æ€»å…ƒç´ æ•°é‡
print(x.numel())  # 3 * 4 * 5 = 60
```

- å†…å­˜ç›¸å…³

```
# æ˜¯å¦è¿ç»­å­˜å‚¨
print(x.is_contiguous())  

# æ˜¯å¦éœ€è¦æ¢¯åº¦
print(x.requires_grad)  

# è·å–æ¢¯åº¦
print(x.grad)  

# æŸ¥çœ‹å­˜å‚¨ä¿¡æ¯
print(x.storage())

```

- ç»´åº¦å˜æ¢

  ```
  # ç»´åº¦å˜æ¢
  x = x.view(12, 5)      # æ”¹å˜å½¢çŠ¶ï¼Œè¦æ±‚è¿ç»­
  x = x.reshape(12, 5)   # æ”¹å˜å½¢çŠ¶ï¼Œæ›´çµæ´»
  
  # ç»´åº¦è½¬ç½®
  x = x.transpose(0, 1)  # äº¤æ¢æŒ‡å®šç»´åº¦
  x = x.permute(2,0,1)   # ä»»æ„é¡ºåºé‡æ’ç»´åº¦
  
  # å¢å‡ç»´åº¦
  x = x.unsqueeze(0)     # å¢åŠ ç»´åº¦
  x = x.squeeze()        # ç§»é™¤å¤§å°ä¸º1çš„ç»´åº¦
  ```



- æ•°æ®è½¬åŒ–

```
# è®¾å¤‡è½¬æ¢
x = x.to('cuda')       # è½¬åˆ°GPU
x = x.cpu()           # è½¬åˆ°CPU

# ç±»å‹è½¬æ¢
x = x.float()         # è½¬ä¸ºfloat
x = x.long()          # è½¬ä¸ºlong
x = x.bool()          # è½¬ä¸ºboolean

# è½¬numpy
numpy_array = x.numpy()
# numpyè½¬tensor
tensor = torch.from_numpy(numpy_array)
```



- å¸¸ç”¨ä¿¡æ¯è·å–

```
# æœ€å¤§æœ€å°å€¼
print(x.max())
print(x.min())

# å‡å€¼æ ‡å‡†å·®
print(x.mean())
print(x.std())

# ç´¢å¼•ç›¸å…³
print(x.argmax())     # æœ€å¤§å€¼ç´¢å¼•
print(x.argmin())     # æœ€å°å€¼ç´¢å¼•
```

