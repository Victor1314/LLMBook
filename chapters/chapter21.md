# 使用输入数据 inputs 进行前向传播，并打印结果
print(sa_v2(inputs))
```



### 因果注意力





#### 因果注意力掩码



因果注意力机制是一种在自注意力机制的基础之上，增加因果注意力掩码和 `dropout`机制的注意力机制。



因果注意力掩码一种剔除序列中往后的token对当前token预测影响的技术, 它能够让下一个token的预测仅基之前的token。 



如何实现因果注意力？ 



在传统的自注意力机制中， 计算权重时，会得到一个n *n的权重W矩阵。

第i行表示，第i个token对于需了中的各个token的权重。 w（i,j）表示第i个token,对于第j个token的权重分数。

对权重矩阵进行对角化处理，将对角线上方的位置对置为0。 如此一来，每个token只能看到自己之前的token。

对角线上方置为0之后，每一行的概率和不再为0，需重新进行 `softmax` 归一化处理。 



代码实现：

创建一个掩码 ：

```
