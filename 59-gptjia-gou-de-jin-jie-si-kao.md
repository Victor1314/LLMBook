# GPT架构的进阶思考

**元**

**反**



Logits每次都需要计算所有，导致会又很多浪费。因此 kv -cache诞生了

**空**



**什么是MOE模型？**
MOE，混合专家模型，是一种神经网络模型。由门控和多个专家网络构成。用于替换前馈神经网络。前馈神经网络是全部参数激活的，也叫做dense模型。而MOE则是先由门控来控制激活哪些专家，可以减少推理时的参数激活量。但MOE模型在训练难度上比Dense模型要高。 



**RMSＮorm是什么？** 

RMSNorm是一种基于均方根的归一化方法。GPT最开始使用的LayerNorm需要先减去均值，再除以方差，使得向量变成均值为０，方差为１的向量。RMSNorm不需要计算均值，也不使用方差来归一化。而是先将计算各项的平方和的均值，开根号，得到均方根。再用各项除以均方根，得到归一化后的值。

RMSＮorm的计算量比常规的LayerNorm要少，逐渐被主流的大模型采用，比如Qwen





