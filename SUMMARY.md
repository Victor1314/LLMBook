* [零基础入门大模型](01-ling-ji-chu-ru-men-da-mo-xing.md)
    * [前言](02-qian-yan.md)
        * [如何使用本书](03-ru-he-shi-yong-ben-shu.md)
        * [qwen2.5介绍](04-qwen2-5jie-shao.md)
        * [LLama-Factory介绍](05-llama-factoryjie-shao.md)
    * [一、快速开始 —  训练你的第一个模型](06-yi-kuai-su-kai-shi-xun-lian-ni-de-di-yi-ge-mo-xing.md)
        * [💻 环境准备](07-huan-jing-zhun-bei.md)
        * [🚀 模型训练](08-mo-xing-xun-lian.md)
            * [预训练(pretrain) —— 让模型学习海量知识](09-yu-xun-lian-pretrain-rang-mo-xing-xue-xi-hai-liang-zhi-shi.md)
            * [监督微调(sft) — 让模型学会说话](10-jian-du-wei-diao-sft-rang-mo-xing-xue-hui-shuo-hua.md)
        * [🌐 模型部署](11-mo-xing-bu-shu.md)
            * [转化为HF格式](12-zhuan-hua-wei-hfge-shi.md)
            * [上传到 Huggingface](13-shang-chuan-dao-huggingface.md)
            * [创建Space，show出你的模型](14-chuang-jian-space-showchu-ni-de-mo-xing.md)
        * [快速开始的进阶思考](15-kuai-su-kai-shi-de-jin-jie-si-kao.md)
        * [小结](16-xiao-jie.md)
    * [二、数据集](17-er-shu-ju-ji.md)
        * [如何对文本进行编码？Tokenizer](18-ru-he-dui-wen-ben-jin-xing-bian-ma-tokenizer.md)
        * [embedding — 把数字变成向量](19-embedding-ba-shu-zi-bian-cheng-xiang-liang.md)
        * [位置编码](20-wei-zhi-bian-ma.md)
        * [构建训练- 目标对](21-gou-jian-xun-lian-mu-biao-dui.md)
        * [如何批量加载和组织数据？](22-ru-he-pi-liang-jia-zai-he-zu-zhi-shu-ju.md)
        * [常见的数据集格式：](23-chang-jian-de-shu-ju-ji-ge-shi.md)
        * [数据集的进阶思考](24-shu-ju-ji-de-jin-jie-si-kao.md)
        * [小结](25-xiao-jie.md)
    * [三、自注意力机制](26-san-zi-zhu-yi-li-ji-zhi.md)
        * [为何Transfomer架构能够胜出？](27-wei-he-transfomerjia-gou-neng-gou-sheng-chu.md)
        * [自注意力机制](28-zi-zhu-yi-li-ji-zhi.md)
            * [权重分数](29-quan-zhong-fen-shu.md)
            * [权重](30-quan-zhong.md)
            * [上下文向量](31-shang-xia-wen-xiang-liang.md)
            * [QKV矩阵](32-qkvju-zhen.md)
        * [因果注意力](33-yin-guo-zhu-yi-li.md)
            * [因果注意力掩码](34-yin-guo-zhu-yi-li-yan-ma.md)
            * [dropout机制](35-dropoutji-zhi.md)
        * [多头注意力](36-duo-tou-zhu-yi-li.md)
        * [注意力机制的进阶思考](37-zhu-yi-li-ji-zhi-de-jin-jie-si-kao.md)
        * [小结](38-xiao-jie.md)
    * [四、实现一个GPT模型](39-si-shi-xian-yi-ge-gptmo-xing.md)
        * [最核心模块— transformer block](40-zui-he-xin-mo-kuai-transformer-block.md)
            * [多头注意力机制](41-duo-tou-zhu-yi-li-ji-zhi.md)
            * [前馈神经网络](42-qian-kui-shen-jing-wang-luo.md)
            * [层归一化](43-ceng-gui-yi-hua.md)
            * [残差连接](44-can-chai-lian-jie.md)
            * [tranformer block代码实现](45-tranformer-blockdai-ma-shi-xian.md)
        * [GPT类实现（数据输入，输出）](46-gptlei-shi-xian-shu-ju-shu-ru-shu-chu.md)
            * [GPT核心有哪些组件？](47-gpthe-xin-you-na-xie-zu-jian.md)
        * [简单文本生成](48-jian-dan-wen-ben-sheng-cheng.md)
        * [GPT架构的进阶思考](49-gptjia-gou-de-jin-jie-si-kao.md)
        * [小结](50-xiao-jie.md)
    * [五、如何训练模型](51-wu-ru-he-xun-lian-mo-xing.md)
        * [如何评估模型的输出？](52-ru-he-ping-gu-mo-xing-de-shu-chu.md)
            * [交叉熵](53-jiao-cha-shang.md)
            * [困惑度](54-kun-huo-du.md)
            * [计算训练集和验证集的损失](55-ji-suan-xun-lian-ji-he-yan-zheng-ji-de-sun-shi.md)
        * [模型训练的基本流程](56-mo-xing-xun-lian-de-ji-ben-liu-cheng.md)
        * [高阶的训练技巧](57-gao-jie-de-xun-lian-ji-qiao.md)
        * [解码策略](58-jie-ma-ce-lue.md)
        * [模型训练的的进阶思考](59-mo-xing-xun-lian-de-de-jin-jie-si-kao.md)
        * [小结](60-xiao-jie.md)
    * [六、如何使用LLamaFactory微调模型](61-liu-ru-he-shi-yong-llamafactorywei-diao-mo-xing.md)
        * [Lora微调](62-lorawei-diao.md)
        * [LLamaFactory的微调流程](63-llamafactoryde-wei-diao-liu-cheng.md)
        * [数据集构建](64-shu-ju-ji-gou-jian.md)
        * [参数设置](65-can-shu-she-zhi.md)
        * [开始训练](66-kai-shi-xun-lian.md)
        * [数据微调的进阶思考](67-shu-ju-wei-diao-de-jin-jie-si-kao.md)
        * [小结](68-xiao-jie.md)
    * [七、附录](69-qi-fu-lu.md)
    * [张量的基本操作](70-zhang-liang-de-ji-ben-cao-zuo.md)
