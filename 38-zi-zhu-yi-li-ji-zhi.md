# 自注意力机制

自注意力机制是什么？  



自注意力机制是一种对**同一序列**内不同token之间的关系编码的技术，也就是能够将token和token之间的关系进行编码。

通过自注意力机制，序列中的token都会得到一个上下文向量（context -vector), 用于表示序列中上下文的信息。



**简单的自注意力机制**不包含**权重矩阵**，仅用于演示注意力机制的简单流程。权重矩阵是模型中可训练的参数。 



简单的自注意力机制的**实现流程**：

遍历序列中的token所对应的向量值。

计算当前向量和其它向量的点积，得到权重分数，权重分数经过 `softmax`函数归一化得到权重值。

序列中的所有向量 * 权重数进行累加，就得到当前token对应的上下文向量。



举例：

序列为：”我爱阅读“

对应的向量：(vector1, vetor2,vector3,vector4 )

当前的token为”我“，那么会用 vector1 *  vectori （i = 1,2,3,4）得到权重分数(score1,score2,score3,score4)

权重分数归一化之后得到(w1,w2,w3,w4)

最终得到，”我“对应的 context_vector1 =   w1 *vecotr1 + w2 * vector2 + w3 * vector3 + w4* vector4。 



在第二章根据token创建embeding的时候，仅考虑了单个toekn以及token在序列中的位置，没有考虑token之间的关系。

自注意力机制弥补了这个缺点，经过自注意力机制得到的上下文向量，能更好的表示token的，让模型获得更好地输出。





